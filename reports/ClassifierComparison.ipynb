{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5206ba44-d4ba-40a8-ba47-3a60b613d5ad",
   "metadata": {},
   "source": [
    "# A notebook for comparing the performance of various classifier implementations\n",
    "#### Note: This notebook takes a very long time to run as TensorFlow was having a lot of trouble integrating with the Great Lakes GPU setup. If you want to play with a specific portion it is recommended you either manually run the cells of interest or extract the relevant portions to another notebook.\n",
    "##### Additional Note: Due to an issue with Great Lakes connections the cell run orders may appear to be slightly off. This is the result of the connection terminating but the notebook continuing to run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c3c231-eda1-4ea8-a95b-73c3299d6f58",
   "metadata": {},
   "source": [
    "## Imports and reading in / generating the data for classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8679f3c9-c63a-4a65-a3c7-f41dea300aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the random state for reproducibility\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42b8fc57-ea48-4018-8786-0b5b66baf9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general purpose imports\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "from path import Path\n",
    "\n",
    "# data manipulation imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# sklearn utility imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import SCORERS\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# sklearn model imports\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# get rid of tensorflow debug output\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# tensorflow imports\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "# get rid of some other annoying warning outputs\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73920347-0e40-41dc-9b68-0c24ce786549",
   "metadata": {},
   "source": [
    "### Set the base path variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19650689-aeb4-4875-86ff-c38eea7a064f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the base path for where you have stored the data generated previously (you must complete this step or things won't run correctly)\n",
    "base_path = Path(\"/nfs/turbo/seas-nhcarter/human_wildlife_interactions/classifier_video_data/\")\n",
    "# set the path to where the cloned repo resides\n",
    "repo_path = Path(\"/nfs/turbo/seas-nhcarter/human_wildlife_interactions/repo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df376398-3e9b-4fa1-8c58-086e00b1ca6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in existing video data matrices\n",
    "v_train_path = Path(base_path / \"train_mat.csv\")\n",
    "v_test_path = Path(base_path / \"test_mat.csv\")\n",
    "v_val_path = Path(base_path / \"val_mat.csv\")\n",
    "v_train_df = pd.read_csv(v_train_path) \n",
    "v_test_df = pd.read_csv(v_test_path)\n",
    "v_val_df = pd.read_csv(v_val_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26c6f36a-21a9-42f5-9b4e-06bcb7b0fff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_v = v_train_df.iloc[:,:-1]\n",
    "y_train_v = v_train_df.iloc[:,-1]\n",
    "X_val_v = v_val_df.iloc[:,:-1]\n",
    "y_val_v = v_val_df.iloc[:,-1]\n",
    "X_test_v = v_test_df.iloc[:,:-1]\n",
    "y_test_v = v_test_df.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "979289e0-2efe-4975-87a9-6a5bdce8b210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:      (2794, 1153); data y value shape:       (2794,)\n",
      "Validation data shape: (494, 1153);  validation y value shape: (494,)\n",
      "Test data shape:       (581, 1153);  test y value shape:       (581,)\n"
     ]
    }
   ],
   "source": [
    "# sanity check the read in data\n",
    "print(\"Train data shape:      {}; data y value shape:       {}\".format(X_train_v.shape, y_train_v.shape))\n",
    "print(\"Validation data shape: {};  validation y value shape: {}\".format(X_val_v.shape, y_val_v.shape))\n",
    "print(\"Test data shape:       {};  test y value shape:       {}\".format(X_test_v.shape, y_test_v.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e33bc73-f9db-496e-81da-3179a0e8b1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the frame pickle files (3.8 gigs, give it a minute)\n",
    "frame_path = Path(base_path / \"frame_features_dict.pkl\")\n",
    "with open(frame_path, 'rb') as file:\n",
    "    frames_raw = pickle.load(file, encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c5cd255-b5e2-46fd-9d8d-3176d42f3009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate training data from frames (we don't store this data as this is the only notebook that needs it and it generates pretty quickly)\n",
    "with open(repo_path / 'human_wildlife_interactions/data/processed/hunting_dict.json') as file:\n",
    "    cluster_results = json.load(file)\n",
    "with open(base_path /  'train_ids.pkl', 'rb') as file:\n",
    "    train_ids = pickle.load(file, encoding = 'utf-8')\n",
    "with open(base_path / 'val_ids.pkl', 'rb') as file:\n",
    "    val_ids = pickle.load(file, encoding = 'utf-8')\n",
    "with open(base_path / 'test_ids.pkl', 'rb') as file:\n",
    "    test_ids = pickle.load(file, encoding = 'utf-8')\n",
    "    \n",
    "    \n",
    "def frame_matrix_generator(num_frames = -1):    \n",
    "    audio_train_frames = []\n",
    "    rgb_train_frames = []\n",
    "    y_train_frames = []\n",
    "\n",
    "    audio_val_frames = []\n",
    "    rgb_val_frames = []\n",
    "    y_val_frames = []\n",
    "\n",
    "    audio_test_frames = []\n",
    "    rgb_test_frames = []\n",
    "    y_test_frames = []\n",
    "    \n",
    "\n",
    "    for video_id in cluster_results.keys():\n",
    "        data = frames_raw[video_id]\n",
    "        y_label = cluster_results[video_id]\n",
    "        if video_id in train_ids:\n",
    "            y_train_frames.append(y_label)\n",
    "            if num_frames != -1:\n",
    "                audio_train_frames.append(np.array(data['audio_lst'][0:num_frames]))\n",
    "                rgb_train_frames.append(np.array(data['rgb_lst'][0:num_frames]))\n",
    "            else:\n",
    "                audio_train_frames.append(np.array(data['audio_lst']))\n",
    "                rgb_train_frames.append(np.array(data['rgb_lst']))\n",
    "        if video_id in val_ids:\n",
    "            y_val_frames.append(y_label)\n",
    "            if num_frames != -1:\n",
    "                audio_val_frames.append(np.array(data['audio_lst'][0:num_frames]))\n",
    "                rgb_val_frames.append(np.array(data['rgb_lst'][0:num_frames]))\n",
    "            else:\n",
    "                audio_val_frames.append(np.array(data['audio_lst']))\n",
    "                rgb_val_frames.append(np.array(data['rgb_lst']))\n",
    "        if video_id in test_ids:\n",
    "            y_test_frames.append(y_label)\n",
    "            if num_frames != -1:\n",
    "                audio_test_frames.append(np.array(data['audio_lst'][0:num_frames]))\n",
    "                rgb_test_frames.append(np.array(data['rgb_lst'][0:num_frames]))\n",
    "            else:\n",
    "                audio_test_frames.append(np.array(data['audio_lst']))\n",
    "                rgb_test_frames.append(np.array(data['rgb_lst']))\n",
    "    return np.array(audio_train_frames), np.array(rgb_train_frames), np.array(audio_val_frames), np.array(rgb_val_frames), np.array(audio_test_frames), np.array(rgb_test_frames), np.array(y_train_frames), np.array(y_val_frames), np.array(y_test_frames)\n",
    "            \n",
    "\n",
    "audio_train_frames, rgb_train_frames, audio_val_frames, rgb_val_frames, audio_test_frames, rgb_test_frames, y_train_frames, y_val_frames, y_test_frames = frame_matrix_generator(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aeef2a8-a369-496e-9aaf-c9b025279907",
   "metadata": {},
   "source": [
    "## SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ce89d11e-aa40-486a-9fbb-c2232ebd0a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: SVC\n",
      "Class 0 Precision:  0.9722814498933902  |  Class 1 Precision: 0.32142857142857145\n",
      "Class 0 Recall:     0.8571428571428571  |  Class 1 Recall:    0.7346938775510204\n",
      "Class 0 FScore:     0.911088911088911  |  Class 1 FScore:    0.4472049689440994\n",
      "Class 0 Support:    532  |  Class 1 Support:   49\n",
      "SVC Accuracy Score: 0.846815834767642\n",
      "SVC ROC/AUC Score:  0.7959183673469388\n",
      "[[456  76]\n",
      " [ 13  36]]\n"
     ]
    }
   ],
   "source": [
    "# best result from GridSearchCV (see videoSVM.ipynb for more information)\n",
    "svc_clf = SVC(C=0.01, class_weight={0: 0.1, 1: 0.9}, kernel='linear', random_state=random_state, probability=True).fit(X_train_v, y_train_v)\n",
    "svc_preds = svc_clf.predict(X_test_v)\n",
    "\n",
    "# results from svc_clf\n",
    "precision, recall, fscore, support = precision_recall_fscore_support(y_test_v, svc_preds)\n",
    "svc_score = svc_clf.score(X_test_v, y_test_v)\n",
    "roc_auc = roc_auc_score(y_test_v, svc_preds)\n",
    "\n",
    "print(\"Classifier: SVC\")\n",
    "print(\"Class 0 Precision:  {}  |  Class 1 Precision: {}\".format(precision[0], precision[1]))\n",
    "print(\"Class 0 Recall:     {}  |  Class 1 Recall:    {}\".format(recall[0], recall[1]))\n",
    "print(\"Class 0 FScore:     {}  |  Class 1 FScore:    {}\".format(fscore[0], fscore[1]))\n",
    "print(\"Class 0 Support:    {}  |  Class 1 Support:   {}\".format(support[0], support[1]))\n",
    "print(\"SVC Accuracy Score: {}\".format(svc_score))\n",
    "print(\"SVC ROC/AUC Score:  {}\".format(roc_auc))\n",
    "print(confusion_matrix(y_test_v,svc_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84273eab-ff40-4fc0-9051-b8a174c438ec",
   "metadata": {},
   "source": [
    "## CNN Implementation - Video Data\n",
    "#### Network Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5be6989d-2e9e-41b6-96ab-ed265d99200a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple matmul CNN approach with class weights based off of Supriya Gadi Patil's CNN implementation\n",
    "# https://github.com/supriya-gdptl/kaggle-youtube8m\n",
    "l2_reg = .00000001\n",
    "# define inputs\n",
    "input_1 = keras.Input(shape=(1024,))\n",
    "input_2 = keras.Input(shape=(128,))\n",
    "\n",
    "# reduce using fully connected layer\n",
    "videoNN = keras.layers.Dense(32, activation=tf.nn.leaky_relu,kernel_regularizer=keras.regularizers.l2(l2_reg))(input_1)\n",
    "audioNN = keras.layers.Dense(32, activation=tf.nn.leaky_relu,kernel_regularizer=keras.regularizers.l2(l2_reg))(input_2)\n",
    "\n",
    "# adjust shape to make everything (32x1) instead of (32,)\n",
    "video_dim = tf.expand_dims(videoNN, -1)\n",
    "audio_dim = tf.expand_dims(audioNN, -1)\n",
    "\n",
    "# transpose audio to enable matmul operation\n",
    "audio_dim = tf.transpose(audio_dim, perm=[0,2,1])\n",
    "\n",
    "# matmul to produce 32x32 result\n",
    "matrix = tf.matmul(video_dim, audio_dim)\n",
    "\n",
    "# need another empty dimension for CNN to work\n",
    "matrix = tf.expand_dims(matrix, -1)\n",
    "\n",
    "# 2 layer CNN with single pooling layer\n",
    "convolution_1 = keras.layers.Conv2D(filters=8, kernel_size=[3,3])(matrix)\n",
    "average_pool = keras.layers.AveragePooling2D(pool_size=2, strides=2)(convolution_1)\n",
    "convolution_2 = keras.layers.Conv2D(filters=4, kernel_size=[3,3])(average_pool)\n",
    "\n",
    "# flatten output layer\n",
    "flattening = keras.layers.Flatten()(convolution_2)\n",
    "\n",
    "# output layer\n",
    "output = keras.layers.Dense(1, activation=tf.nn.sigmoid, kernel_regularizer=keras.regularizers.l2(l2_reg))(flattening)\n",
    "\n",
    "# build the graph\n",
    "cnn_model = keras.Model(inputs=[input_1,input_2], outputs=[output])\n",
    "cnn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[keras.metrics.AUC()])\n",
    "\n",
    "# grab the initial weights so we can reset while changing parameters\n",
    "initial_weights = cnn_model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63e1344d-98e8-41e6-a141-24d24384b6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to split the data up to match the inputs specified above\n",
    "train_video = X_train_v.iloc[:, :1024]\n",
    "train_audio = X_train_v.iloc[:,1024:-1]\n",
    "test_video = X_test_v.iloc[:, :1024]\n",
    "test_audio = X_test_v.iloc[:, 1024:-1]\n",
    "val_video = X_val_v.iloc[:, :1024]\n",
    "val_audio = X_val_v.iloc[:, 1024:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3d3720-ec05-4403-8fa2-209ccaf404e8",
   "metadata": {},
   "source": [
    "#### Testing Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e10c42a-d178-4ae8-8e59-cffd380a27fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0 weight: 0.01, Class 1 weight0.99\n",
      "88/88 [==============================] - 2s 15ms/step - loss: 0.0779 - auc_1: 0.5106 - val_loss: 1.0003 - val_auc_1: 0.5306\n",
      "Class 0 weight: 0.02, Class 1 weight0.98\n",
      "88/88 [==============================] - 2s 15ms/step - loss: 0.0958 - auc_2: 0.5196 - val_loss: 0.8993 - val_auc_2: 0.6082\n",
      "Class 0 weight: 0.03, Class 1 weight0.97\n",
      "88/88 [==============================] - 2s 15ms/step - loss: 0.1188 - auc_3: 0.5371 - val_loss: 0.8024 - val_auc_3: 0.6329\n",
      "Class 0 weight: 0.04, Class 1 weight0.96\n",
      "88/88 [==============================] - 2s 15ms/step - loss: 0.1378 - auc_4: 0.5141 - val_loss: 0.7910 - val_auc_4: 0.6014\n",
      "Class 0 weight: 0.05, Class 1 weight0.95\n",
      "88/88 [==============================] - 2s 15ms/step - loss: 0.1507 - auc_5: 0.5273 - val_loss: 0.7022 - val_auc_5: 0.7064\n",
      "Class 0 weight: 0.06, Class 1 weight0.94\n",
      "88/88 [==============================] - 2s 15ms/step - loss: 0.1543 - auc_6: 0.5208 - val_loss: 0.6762 - val_auc_6: 0.6605\n",
      "Class 0 weight: 0.07, Class 1 weight0.9299999999999999\n",
      "88/88 [==============================] - 2s 15ms/step - loss: 0.1687 - auc_7: 0.5544 - val_loss: 0.6850 - val_auc_7: 0.6578\n",
      "Class 0 weight: 0.08, Class 1 weight0.92\n",
      "88/88 [==============================] - 2s 15ms/step - loss: 0.1805 - auc_8: 0.5575 - val_loss: 0.6448 - val_auc_8: 0.6832\n",
      "Class 0 weight: 0.09, Class 1 weight0.91\n",
      "88/88 [==============================] - 2s 15ms/step - loss: 0.1880 - auc_9: 0.5344 - val_loss: 0.6692 - val_auc_9: 0.6111\n",
      "Class 0 weight: 0.1, Class 1 weight0.9\n",
      "88/88 [==============================] - 2s 19ms/step - loss: 0.1827 - auc_10: 0.5393 - val_loss: 0.6412 - val_auc_10: 0.6987\n"
     ]
    }
   ],
   "source": [
    "# testing different weights\n",
    "weight_vals = [.01,.02,.03,.04,.05,.06,.07,.08,.09,.1]\n",
    "for val in weight_vals:\n",
    "    # reset the model back to starting weights between each run to avoid inadvertently testing weighted epochs instead\n",
    "    cnn_model.set_weights(initial_weights)\n",
    "    w_0 = val\n",
    "    w_1 = 1 - w_0\n",
    "    print(\"Class 0 weight: {}, Class 1 weight{}\".format(w_0, w_1))\n",
    "    cnn_model = keras.Model(inputs=[input_1,input_2], outputs=[output])\n",
    "    cnn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[keras.metrics.AUC()])\n",
    "    cnn_model.fit(x=[train_video, train_audio], y=y_train_v, class_weight={0: w_0, 1: w_1},validation_data=([val_video, val_audio], y_val_v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effe0f45-b494-4430-8b84-8d0e4f6ca95c",
   "metadata": {},
   "source": [
    "#### Run the best model above and generate some metrics\n",
    "##### You will need to do some analysis on the runs above and manually adjust the class_weight parameter below. This is due to a combination of random initialization behaviors between notebook runs (TensorFlow will change the values everytime the kernel restarts) and a judgement call between training and validation performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8cf0215d-111f-40ab-b27f-2deaf4f491e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 2s 15ms/step - loss: 0.2007 - auc_11: 0.5334 - val_loss: 0.5994 - val_auc_11: 0.6714\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14a2b5dde310>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in this instance the best balance seems to have been achieved at .1 / .9 so the weights are adjusted to reflect\n",
    "cnn_model.set_weights(initial_weights)\n",
    "cnn_model = keras.Model(inputs=[input_1,input_2], outputs=[output])\n",
    "cnn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[keras.metrics.AUC()])\n",
    "cnn_model.fit(x=[train_video, train_audio], y=y_train_v, class_weight={0: .1, 1: .9},validation_data=([val_video, val_audio], y_val_v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f46ad777-83a4-4edd-812c-fc9f2da4d0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 5ms/step - loss: 0.5851 - auc_11: 0.6879\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5850654244422913, 0.6878932118415833]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model.evaluate([test_video, test_audio], y_test_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2235aaa-9b16-41bf-a54f-94c7342ff358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 5ms/step\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 0.5851 - auc_11: 0.6879\n",
      "Classifier: CNN\n",
      "Class 0 Precision:  0.9156626506024096  |  Class 1 Precision: 0.0\n",
      "Class 0 Recall:     1.0  |  Class 1 Recall:    0.0\n",
      "Class 0 FScore:     0.9559748427672956  |  Class 1 FScore:    0.0\n",
      "Class 0 Support:    532  |  Class 1 Support:   49\n",
      "CNN Accuracy Score: 0.9156626506024096\n",
      "CNN ROC/AUC Score:  0.6878932118415833\n",
      "[[532   0]\n",
      " [ 49   0]]\n"
     ]
    }
   ],
   "source": [
    "cnn_raw_preds = cnn_model.predict([test_video, test_audio])\n",
    "cnn_preds = []\n",
    "for probs in cnn_raw_preds:\n",
    "    cnn_preds.append(np.argmax(probs))\n",
    "precision, recall, fscore, support = precision_recall_fscore_support(y_test_v, cnn_preds)\n",
    "accuracy = accuracy_score(y_test_v, cnn_preds)\n",
    "cnn_loss, cnn_auc = cnn_model.evaluate([test_video, test_audio], y_test_v)\n",
    "print(\"Classifier: CNN\")\n",
    "print(\"Class 0 Precision:  {}  |  Class 1 Precision: {}\".format(precision[0], precision[1]))\n",
    "print(\"Class 0 Recall:     {}  |  Class 1 Recall:    {}\".format(recall[0], recall[1]))\n",
    "print(\"Class 0 FScore:     {}  |  Class 1 FScore:    {}\".format(fscore[0], fscore[1]))\n",
    "print(\"Class 0 Support:    {}  |  Class 1 Support:   {}\".format(support[0], support[1]))\n",
    "print(\"CNN Accuracy Score: {}\".format(accuracy))\n",
    "print(\"CNN ROC/AUC Score:  {}\".format(cnn_auc))\n",
    "print(confusion_matrix(y_test_v,cnn_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0da446-3566-49df-b603-79ac8c08cfbd",
   "metadata": {},
   "source": [
    "#### Testing class weights with early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b76ec5c-16fb-4399-8704-bd3c1e2600bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01 0.99\n",
      "88/88 [==============================] - 2s 15ms/step - loss: 0.0760 - auc: 0.5135 - val_loss: 1.0498 - val_auc: 0.5131\n",
      "0.02 0.98\n",
      "88/88 [==============================] - 2s 15ms/step - loss: 0.0913 - auc_1: 0.5300 - val_loss: 1.1115 - val_auc_1: 0.5403\n",
      "0.03 0.97\n",
      "88/88 [==============================] - 2s 15ms/step - loss: 0.1178 - auc_2: 0.5128 - val_loss: 0.8251 - val_auc_2: 0.5677\n",
      "0.04 0.96\n",
      "88/88 [==============================] - 2s 15ms/step - loss: 0.1487 - auc_3: 0.5472 - val_loss: 0.7704 - val_auc_3: 0.6426\n",
      "0.05 0.95\n",
      "88/88 [==============================] - 2s 15ms/step - loss: 0.1606 - auc_4: 0.5261 - val_loss: 0.7605 - val_auc_4: 0.6424\n",
      "0.06 0.94\n",
      "88/88 [==============================] - 3s 15ms/step - loss: 0.1545 - auc_5: 0.5184 - val_loss: 0.8368 - val_auc_5: 0.5965\n",
      "0.07 0.9299999999999999\n",
      "88/88 [==============================] - 2s 15ms/step - loss: 0.1710 - auc_6: 0.5275 - val_loss: 0.7091 - val_auc_6: 0.6772\n",
      "0.08 0.92\n",
      "88/88 [==============================] - 2s 15ms/step - loss: 0.1661 - auc_7: 0.5615 - val_loss: 0.6685 - val_auc_7: 0.7176\n",
      "0.09 0.91\n",
      "88/88 [==============================] - 2s 15ms/step - loss: 0.1885 - auc_8: 0.5251 - val_loss: 0.6449 - val_auc_8: 0.5932\n",
      "0.1 0.9\n",
      "88/88 [==============================] - 2s 15ms/step - loss: 0.1906 - auc_9: 0.5293 - val_loss: 0.6424 - val_auc_9: 0.6990\n"
     ]
    }
   ],
   "source": [
    "# same as previous model but with early stopping\n",
    "weight_vals = [.01,.02,.03,.04,.05,.06,.07,.08,.09,.1]\n",
    "callback = keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "keras.backend.clear_session()\n",
    "for val in weight_vals:\n",
    "    cnn_model.set_weights(initial_weights)\n",
    "    w_0 = val\n",
    "    w_1 = 1 - w_0\n",
    "    print(w_0, w_1)\n",
    "    cnn_model = keras.Model(inputs=[input_1,input_2], outputs=[output])\n",
    "    cnn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[keras.metrics.AUC()])\n",
    "    cnn_model.fit(x=[train_video, train_audio], y=y_train_v, class_weight={0: w_0, 1: w_1},validation_data=([val_video, val_audio], y_val_v), callbacks=[callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5fc5cc-a5f0-4b6c-a3eb-ad1832fff205",
   "metadata": {},
   "source": [
    "#### Run the best model above and generate some metrics\n",
    "##### You will need to do some analysis on the runs above and manually adjust the class_weight parameter below. This is due to a combination of random initialization behaviors between notebook runs (TensorFlow will change the values everytime the kernel restarts) and a judgement call between training and validation performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dfb21db0-aea0-4d16-8454-8273c7347f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 2s 15ms/step - loss: 0.1878 - auc_10: 0.5283 - val_loss: 0.6999 - val_auc_10: 0.6495\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14a2b02ef700>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in this case it seems the best performance was .08 / .92, weights have been adjusted accordingly\n",
    "cnn_model.set_weights(initial_weights)\n",
    "cnn_model = keras.Model(inputs=[input_1,input_2], outputs=[output])\n",
    "cnn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[keras.metrics.AUC()])\n",
    "cnn_model.fit(x=[train_video, train_audio], y=y_train_v, class_weight={0: .08, 1: .92},validation_data=([val_video, val_audio], y_val_v), callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ea32310-34d3-47f5-87e2-3b5f0187a666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 5ms/step\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 0.7065 - auc_10: 0.6577\n",
      "Classifier: CNN\n",
      "Class 0 Precision:  0.9156626506024096  |  Class 1 Precision: 0.0\n",
      "Class 0 Recall:     1.0  |  Class 1 Recall:    0.0\n",
      "Class 0 FScore:     0.9559748427672956  |  Class 1 FScore:    0.0\n",
      "Class 0 Support:    532  |  Class 1 Support:   49\n",
      "CNN Accuracy Score: 0.9156626506024096\n",
      "CNN ROC/AUC Score:  0.6577221155166626\n",
      "[[532   0]\n",
      " [ 49   0]]\n"
     ]
    }
   ],
   "source": [
    "cnn_raw_preds = cnn_model.predict([test_video, test_audio])\n",
    "cnn_preds = []\n",
    "for probs in cnn_raw_preds:\n",
    "    cnn_preds.append(np.argmax(probs))\n",
    "precision, recall, fscore, support = precision_recall_fscore_support(y_test_v, cnn_preds)\n",
    "accuracy = accuracy_score(y_test_v, cnn_preds)\n",
    "cnn_loss, cnn_auc = cnn_model.evaluate([test_video, test_audio], y_test_v)\n",
    "print(\"Classifier: CNN\")\n",
    "print(\"Class 0 Precision:  {}  |  Class 1 Precision: {}\".format(precision[0], precision[1]))\n",
    "print(\"Class 0 Recall:     {}  |  Class 1 Recall:    {}\".format(recall[0], recall[1]))\n",
    "print(\"Class 0 FScore:     {}  |  Class 1 FScore:    {}\".format(fscore[0], fscore[1]))\n",
    "print(\"Class 0 Support:    {}  |  Class 1 Support:   {}\".format(support[0], support[1]))\n",
    "print(\"CNN Accuracy Score: {}\".format(accuracy))\n",
    "print(\"CNN ROC/AUC Score:  {}\".format(cnn_auc))\n",
    "print(confusion_matrix(y_test_v,cnn_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd654ee8-71a1-4058-8b5e-2f5c50ab0d4f",
   "metadata": {},
   "source": [
    "#### Testing epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb93c29b-d0af-4152-995f-d38773784fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "88/88 [==============================] - 2s 15ms/step - loss: 0.1883 - auc_11: 0.5205 - val_loss: 0.6120 - val_auc_11: 0.6776\n",
      "Epoch 2/10\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.1217 - auc_11: 0.6177 - val_loss: 0.6734 - val_auc_11: 0.7039\n",
      "Epoch 3/10\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.1102 - auc_11: 0.6770 - val_loss: 0.6358 - val_auc_11: 0.7121\n",
      "Epoch 4/10\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.0993 - auc_11: 0.7582 - val_loss: 0.6005 - val_auc_11: 0.7161\n",
      "Epoch 5/10\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.0929 - auc_11: 0.7911 - val_loss: 0.6568 - val_auc_11: 0.7193\n",
      "Epoch 6/10\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.0878 - auc_11: 0.8208 - val_loss: 0.6636 - val_auc_11: 0.7735\n",
      "Epoch 7/10\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.0762 - auc_11: 0.8800 - val_loss: 0.5962 - val_auc_11: 0.7861\n",
      "Epoch 8/10\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.0687 - auc_11: 0.9057 - val_loss: 0.5142 - val_auc_11: 0.7887\n",
      "Epoch 9/10\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.0585 - auc_11: 0.9315 - val_loss: 0.5443 - val_auc_11: 0.7963\n",
      "Epoch 10/10\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.0503 - auc_11: 0.9501 - val_loss: 0.5490 - val_auc_11: 0.8017\n",
      "Epoch 1/11\n",
      "88/88 [==============================] - 3s 15ms/step - loss: 0.2135 - auc_12: 0.5295 - val_loss: 0.6212 - val_auc_12: 0.6361\n",
      "Epoch 2/11\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.1236 - auc_12: 0.5951 - val_loss: 0.6686 - val_auc_12: 0.7004\n",
      "Epoch 3/11\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.1090 - auc_12: 0.6889 - val_loss: 0.6369 - val_auc_12: 0.7039\n",
      "Epoch 4/11\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.1010 - auc_12: 0.7459 - val_loss: 0.6201 - val_auc_12: 0.7345\n",
      "Epoch 5/11\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.0910 - auc_12: 0.8099 - val_loss: 0.6180 - val_auc_12: 0.7629\n",
      "Epoch 6/11\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.0981 - auc_12: 0.7898 - val_loss: 0.5930 - val_auc_12: 0.7873\n",
      "Epoch 7/11\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.0772 - auc_12: 0.8772 - val_loss: 0.5701 - val_auc_12: 0.7821\n",
      "Epoch 8/11\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.0699 - auc_12: 0.9037 - val_loss: 0.5573 - val_auc_12: 0.7972\n",
      "Epoch 9/11\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.0594 - auc_12: 0.9292 - val_loss: 0.5411 - val_auc_12: 0.7959\n",
      "Epoch 10/11\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.0545 - auc_12: 0.9416 - val_loss: 0.5747 - val_auc_12: 0.7882\n",
      "Epoch 11/11\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.0509 - auc_12: 0.9484 - val_loss: 0.5273 - val_auc_12: 0.8145\n",
      "Epoch 1/12\n",
      "88/88 [==============================] - 2s 15ms/step - loss: 0.2128 - auc_13: 0.5171 - val_loss: 0.6752 - val_auc_13: 0.6323\n",
      "Epoch 2/12\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.1300 - auc_13: 0.5875 - val_loss: 0.6939 - val_auc_13: 0.6344\n",
      "Epoch 3/12\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.1149 - auc_13: 0.6474 - val_loss: 0.6184 - val_auc_13: 0.6965\n",
      "Epoch 4/12\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.1054 - auc_13: 0.7157 - val_loss: 0.6074 - val_auc_13: 0.7192\n",
      "Epoch 5/12\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.0950 - auc_13: 0.7969 - val_loss: 0.6172 - val_auc_13: 0.7350\n",
      "Epoch 6/12\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.0873 - auc_13: 0.8244 - val_loss: 0.6244 - val_auc_13: 0.7269\n",
      "Epoch 7/12\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.0832 - auc_13: 0.8488 - val_loss: 0.5401 - val_auc_13: 0.7275\n",
      "Epoch 8/12\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.0719 - auc_13: 0.8955 - val_loss: 0.5899 - val_auc_13: 0.7746\n",
      "Epoch 9/12\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.0616 - auc_13: 0.9250 - val_loss: 0.5481 - val_auc_13: 0.7856\n",
      "Epoch 10/12\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.0587 - auc_13: 0.9277 - val_loss: 0.5490 - val_auc_13: 0.7894\n",
      "Epoch 11/12\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.0495 - auc_13: 0.9520 - val_loss: 0.5342 - val_auc_13: 0.7754\n",
      "Epoch 12/12\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.0459 - auc_13: 0.9573 - val_loss: 0.5129 - val_auc_13: 0.7584\n",
      "Epoch 1/13\n",
      "88/88 [==============================] - 2s 15ms/step - loss: 0.2271 - auc_14: 0.5034 - val_loss: 0.6438 - val_auc_14: 0.6813\n",
      "Epoch 2/13\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.1215 - auc_14: 0.6188 - val_loss: 0.6214 - val_auc_14: 0.6863\n",
      "Epoch 3/13\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.1120 - auc_14: 0.6669 - val_loss: 0.6445 - val_auc_14: 0.7373\n",
      "Epoch 4/13\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.1019 - auc_14: 0.7341 - val_loss: 0.6266 - val_auc_14: 0.7346\n",
      "Epoch 5/13\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.0921 - auc_14: 0.8079 - val_loss: 0.6127 - val_auc_14: 0.7448\n",
      "Epoch 6/13\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.0820 - auc_14: 0.8523 - val_loss: 0.5729 - val_auc_14: 0.7645\n",
      "Epoch 7/13\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.0738 - auc_14: 0.8850 - val_loss: 0.5414 - val_auc_14: 0.7885\n",
      "Epoch 8/13\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.0673 - auc_14: 0.9070 - val_loss: 0.5215 - val_auc_14: 0.7980\n",
      "Epoch 9/13\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.0608 - auc_14: 0.9262 - val_loss: 0.5618 - val_auc_14: 0.7920\n",
      "Epoch 10/13\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.0664 - auc_14: 0.9108 - val_loss: 0.4987 - val_auc_14: 0.7932\n",
      "Epoch 11/13\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.0507 - auc_14: 0.9510 - val_loss: 0.5586 - val_auc_14: 0.7990\n",
      "Epoch 12/13\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.0418 - auc_14: 0.9668 - val_loss: 0.4810 - val_auc_14: 0.7920\n",
      "Epoch 13/13\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.0403 - auc_14: 0.9666 - val_loss: 0.5457 - val_auc_14: 0.7850\n",
      "Epoch 1/14\n",
      "88/88 [==============================] - 2s 15ms/step - loss: 0.2242 - auc_15: 0.4897 - val_loss: 0.6525 - val_auc_15: 0.6397\n",
      "Epoch 2/14\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.1204 - auc_15: 0.6163 - val_loss: 0.6056 - val_auc_15: 0.7032\n",
      "Epoch 3/14\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.1139 - auc_15: 0.6657 - val_loss: 0.6586 - val_auc_15: 0.7314\n",
      "Epoch 4/14\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.1010 - auc_15: 0.7375 - val_loss: 0.6007 - val_auc_15: 0.7421\n",
      "Epoch 5/14\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.0955 - auc_15: 0.7888 - val_loss: 0.6054 - val_auc_15: 0.7506\n",
      "Epoch 6/14\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.0843 - auc_15: 0.8447 - val_loss: 0.5954 - val_auc_15: 0.7421\n",
      "Epoch 7/14\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.0836 - auc_15: 0.8450 - val_loss: 0.5845 - val_auc_15: 0.7724\n",
      "Epoch 8/14\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.0689 - auc_15: 0.9045 - val_loss: 0.5927 - val_auc_15: 0.7810\n",
      "Epoch 9/14\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.0620 - auc_15: 0.9242 - val_loss: 0.5679 - val_auc_15: 0.7915\n",
      "Epoch 10/14\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.0565 - auc_15: 0.9340 - val_loss: 0.5300 - val_auc_15: 0.7950\n",
      "Epoch 11/14\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.0466 - auc_15: 0.9573 - val_loss: 0.4851 - val_auc_15: 0.8001\n",
      "Epoch 12/14\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.0416 - auc_15: 0.9660 - val_loss: 0.5027 - val_auc_15: 0.7959\n",
      "Epoch 13/14\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.0453 - auc_15: 0.9571 - val_loss: 0.5286 - val_auc_15: 0.7998\n",
      "Epoch 14/14\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.0432 - auc_15: 0.9604 - val_loss: 0.4710 - val_auc_15: 0.8060\n",
      "Epoch 1/15\n",
      "88/88 [==============================] - 2s 15ms/step - loss: 0.1956 - auc_16: 0.5256 - val_loss: 0.7248 - val_auc_16: 0.6459\n",
      "Epoch 2/15\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.1237 - auc_16: 0.5968 - val_loss: 0.6528 - val_auc_16: 0.6785\n",
      "Epoch 3/15\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.1102 - auc_16: 0.6668 - val_loss: 0.6568 - val_auc_16: 0.7377\n",
      "Epoch 4/15\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.1070 - auc_16: 0.7179 - val_loss: 0.5801 - val_auc_16: 0.7126\n",
      "Epoch 5/15\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.0935 - auc_16: 0.7825 - val_loss: 0.5986 - val_auc_16: 0.7427\n",
      "Epoch 6/15\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.0862 - auc_16: 0.8365 - val_loss: 0.5705 - val_auc_16: 0.7634\n",
      "Epoch 7/15\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.0768 - auc_16: 0.8767 - val_loss: 0.5494 - val_auc_16: 0.7592\n",
      "Epoch 8/15\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.0679 - auc_16: 0.9060 - val_loss: 0.5756 - val_auc_16: 0.7741\n",
      "Epoch 9/15\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.0580 - auc_16: 0.9368 - val_loss: 0.5431 - val_auc_16: 0.7934\n",
      "Epoch 10/15\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.0562 - auc_16: 0.9376 - val_loss: 0.5703 - val_auc_16: 0.7848\n",
      "Epoch 11/15\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.0525 - auc_16: 0.9428 - val_loss: 0.5033 - val_auc_16: 0.7807\n",
      "Epoch 12/15\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.0446 - auc_16: 0.9584 - val_loss: 0.5008 - val_auc_16: 0.7976\n",
      "Epoch 13/15\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.0399 - auc_16: 0.9671 - val_loss: 0.5137 - val_auc_16: 0.7940\n",
      "Epoch 14/15\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.0413 - auc_16: 0.9643 - val_loss: 0.5245 - val_auc_16: 0.7919\n",
      "Epoch 15/15\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.0295 - auc_16: 0.9830 - val_loss: 0.4754 - val_auc_16: 0.7913\n"
     ]
    }
   ],
   "source": [
    "# epoch experiments\n",
    "epoch_list = [10,11,12,13,14,15] # the best AUC values in our testing came from this range, feel free to modify as desired\n",
    "for epoch in epoch_list:\n",
    "    cnn_model.set_weights(initial_weights)\n",
    "    cnn_model = keras.Model(inputs=[input_1,input_2], outputs=[output])\n",
    "    cnn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[keras.metrics.AUC()])\n",
    "    cnn_model.fit(x=[train_video, train_audio], y=y_train_v, class_weight={0:.1, 1:.9},validation_data=([val_video, val_audio], y_val_v), callbacks=[callback],epochs=epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100befa9-55fc-46e9-a7d7-49c2e5faf42a",
   "metadata": {},
   "source": [
    "#### Run the best model above and generate some metrics\n",
    "##### You will need to do some analysis on the runs above and manually adjust the class_weight parameter below. This is due to a combination of random initialization behaviors between notebook runs (TensorFlow will change the values everytime the kernel restarts) and a judgement call between training and validation performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7cb418b-be72-43ae-9298-3483bad757cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/11\n",
      "88/88 [==============================] - 2s 15ms/step - loss: 0.1973 - auc_17: 0.5418 - val_loss: 0.6869 - val_auc_17: 0.6485\n",
      "Epoch 2/11\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.1259 - auc_17: 0.5948 - val_loss: 0.6659 - val_auc_17: 0.6984\n",
      "Epoch 3/11\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.1121 - auc_17: 0.6694 - val_loss: 0.6023 - val_auc_17: 0.7246\n",
      "Epoch 4/11\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.0979 - auc_17: 0.7698 - val_loss: 0.6245 - val_auc_17: 0.7112\n",
      "Epoch 5/11\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.0900 - auc_17: 0.8109 - val_loss: 0.6234 - val_auc_17: 0.7423\n",
      "Epoch 6/11\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.0856 - auc_17: 0.8436 - val_loss: 0.6171 - val_auc_17: 0.7497\n",
      "Epoch 7/11\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.0739 - auc_17: 0.8848 - val_loss: 0.6374 - val_auc_17: 0.7614\n",
      "Epoch 8/11\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.0701 - auc_17: 0.8962 - val_loss: 0.5746 - val_auc_17: 0.7758\n",
      "Epoch 9/11\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.0625 - auc_17: 0.9169 - val_loss: 0.5138 - val_auc_17: 0.7797\n",
      "Epoch 10/11\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.0533 - auc_17: 0.9450 - val_loss: 0.5238 - val_auc_17: 0.7855\n",
      "Epoch 11/11\n",
      "88/88 [==============================] - 1s 13ms/step - loss: 0.0463 - auc_17: 0.9574 - val_loss: 0.4648 - val_auc_17: 0.7930\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14a2c88ceaf0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in this run it seems as though 11 epochs had the best results (the previous run occurred at 13), so the epochs have been set to 11\n",
    "cnn_model.set_weights(initial_weights)\n",
    "\n",
    "cnn_model = keras.Model(inputs=[input_1,input_2], outputs=[output])\n",
    "cnn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[keras.metrics.AUC()])\n",
    "cnn_model.fit(x=[train_video, train_audio], y=y_train_v, class_weight={0:.1, 1:.9},validation_data=([val_video, val_audio], y_val_v), callbacks=[callback],epochs=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35431b4a-01cd-4f6f-b8ed-04a9971f91c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 5ms/step\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 0.3928 - auc_17: 0.8291\n",
      "Classifier: CNN\n",
      "Class 0 Precision:  0.9156626506024096  |  Class 1 Precision: 0.0\n",
      "Class 0 Recall:     1.0  |  Class 1 Recall:    0.0\n",
      "Class 0 FScore:     0.9559748427672956  |  Class 1 FScore:    0.0\n",
      "Class 0 Support:    532  |  Class 1 Support:   49\n",
      "CNN Accuracy Score: 0.9156626506024096\n",
      "CNN ROC/AUC Score:  0.8291007876396179\n",
      "[[532   0]\n",
      " [ 49   0]]\n"
     ]
    }
   ],
   "source": [
    "cnn_raw_preds = cnn_model.predict([test_video, test_audio])\n",
    "cnn_preds = []\n",
    "for probs in cnn_raw_preds:\n",
    "    cnn_preds.append(np.argmax(probs))\n",
    "precision, recall, fscore, support = precision_recall_fscore_support(y_test_v, cnn_preds)\n",
    "accuracy = accuracy_score(y_test_v, cnn_preds)\n",
    "cnn_loss, cnn_auc = cnn_model.evaluate([test_video, test_audio], y_test_v)\n",
    "print(\"Classifier: CNN\")\n",
    "print(\"Class 0 Precision:  {}  |  Class 1 Precision: {}\".format(precision[0], precision[1]))\n",
    "print(\"Class 0 Recall:     {}  |  Class 1 Recall:    {}\".format(recall[0], recall[1]))\n",
    "print(\"Class 0 FScore:     {}  |  Class 1 FScore:    {}\".format(fscore[0], fscore[1]))\n",
    "print(\"Class 0 Support:    {}  |  Class 1 Support:   {}\".format(support[0], support[1]))\n",
    "print(\"CNN Accuracy Score: {}\".format(accuracy))\n",
    "print(\"CNN ROC/AUC Score:  {}\".format(cnn_auc))\n",
    "print(confusion_matrix(y_test_v,cnn_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce04ac3a-19a4-47cf-9f37-4ef9b3f1959c",
   "metadata": {},
   "source": [
    "## Fully Connected Neural Network - Video Data\n",
    "#### Network Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "09e21505-b66e-4fca-b989-4eb26195ec1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model design based off of Roberto Chavez's paper\n",
    "# https://github.com/rchavezj/Label_YT_Videos\n",
    "\n",
    "# audio input with fully connected layers\n",
    "audio_input = keras.Input(shape=(128,))\n",
    "audio_fc1 = keras.layers.Dense(512, activation='relu')(audio_input)\n",
    "audio_fc2 = keras.layers.Dense(1024, activation='relu')(audio_fc1)\n",
    "audio_fc3 = keras.layers.Dense(4096, activation='relu')(audio_fc2)\n",
    "audio_fc4 = keras.layers.Dense(8192, activation='relu')(audio_fc3)\n",
    "audio_fc5 = keras.layers.Dense(4096, activation='relu')(audio_fc4)\n",
    "# video input with fully connected layers\n",
    "video_input = keras.Input(shape=(1024,))\n",
    "video_fc1 = keras.layers.Dense(512, activation='relu')(video_input)\n",
    "video_fc2 = keras.layers.Dense(1024, activation='relu')(video_fc1)\n",
    "video_fc3 = keras.layers.Dense(4096, activation='relu')(video_fc2)\n",
    "video_fc4 = keras.layers.Dense(8192, activation='relu')(video_fc3)\n",
    "video_fc5 = keras.layers.Dense(4096, activation='relu')(video_fc4)\n",
    "# merge data and pass to fully connected layer\n",
    "nn_merge = keras.layers.concatenate([audio_fc5, video_fc5])\n",
    "nn_fc = keras.layers.Dense(4096, activation='relu')(nn_merge) \n",
    "# output layer\n",
    "nn_output = keras.layers.Dense(1, activation=tf.nn.sigmoid,name='nn_output')(nn_fc)\n",
    "# compile model\n",
    "nn_model = keras.Model(inputs=[audio_input, video_input],outputs=[nn_output])\n",
    "nn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[keras.metrics.AUC()])\n",
    "# grab the initial weights to reset the model between parameter changes\n",
    "nn_weights = nn_model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35716c8e-5ddf-4799-afce-10d77825a69b",
   "metadata": {},
   "source": [
    "### Testing Class Weights (Long Run Time Warning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "26df484e-be80-4ecd-b882-f755129327ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01 0.99\n",
      "88/88 [==============================] - 443s 5s/step - loss: 0.6741 - auc_19: 0.5004 - val_loss: 2.5162 - val_auc_19: 0.5359\n",
      "0.02 0.98\n",
      "88/88 [==============================] - 467s 5s/step - loss: 0.7975 - auc_20: 0.4930 - val_loss: 1.0037 - val_auc_20: 0.6229\n",
      "0.03 0.97\n",
      "88/88 [==============================] - 468s 5s/step - loss: 1.1930 - auc_21: 0.5084 - val_loss: 0.9017 - val_auc_21: 0.5465\n",
      "0.04 0.96\n",
      "88/88 [==============================] - 456s 5s/step - loss: 4.9077 - auc_22: 0.5000 - val_loss: 1.0155 - val_auc_22: 0.4518\n",
      "0.05 0.95\n",
      "88/88 [==============================] - 462s 5s/step - loss: 1.5441 - auc_23: 0.5289 - val_loss: 1.2295 - val_auc_23: 0.5200\n",
      "0.06 0.94\n",
      "88/88 [==============================] - 461s 5s/step - loss: 0.7763 - auc_24: 0.4958 - val_loss: 0.6893 - val_auc_24: 0.5464\n",
      "0.07 0.9299999999999999\n",
      "88/88 [==============================] - 451s 5s/step - loss: 1.5971 - auc_25: 0.5036 - val_loss: 0.7093 - val_auc_25: 0.4682\n",
      "0.08 0.92\n",
      "88/88 [==============================] - 452s 5s/step - loss: 2.2313 - auc_26: 0.4960 - val_loss: 0.7228 - val_auc_26: 0.4601\n",
      "0.09 0.91\n",
      "88/88 [==============================] - 452s 5s/step - loss: 2.7917 - auc_27: 0.4893 - val_loss: 0.7170 - val_auc_27: 0.5319\n",
      "0.1 0.9\n",
      "88/88 [==============================] - 461s 5s/step - loss: 2.5124 - auc_28: 0.4873 - val_loss: 0.6835 - val_auc_28: 0.5170\n"
     ]
    }
   ],
   "source": [
    "for val in weight_vals:\n",
    "    w_0 = val\n",
    "    w_1 = 1 - w_0\n",
    "    print(w_0, w_1)\n",
    "    nn_model.set_weights(nn_weights)\n",
    "    nn_model = keras.Model(inputs=[audio_input, video_input],outputs=[nn_output])\n",
    "    nn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[keras.metrics.AUC()])\n",
    "    nn_model.fit(x=[train_audio, train_video], y=y_train_v, class_weight={0: w_0, 1: w_1},validation_data=([val_audio,val_video], y_val_v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f2b4cd-79db-4e5c-92e5-f0e83a8135e1",
   "metadata": {},
   "source": [
    "#### Run the best model above and generate some metrics\n",
    "##### You will need to do some analysis on the runs above and manually adjust the class_weight parameter below. This is due to a combination of random initialization behaviors between notebook runs (TensorFlow will change the values everytime the kernel restarts) and a judgement call between training and validation performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67233ef-dc8a-47df-8b3f-c46a7e421ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 463s 5s/step - loss: 1.5598 - auc: 0.5248 - val_loss: 0.7399 - val_auc: 0.5347\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14a2c6674910>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this one is kind of tricky to assess the \"best\" but we have chosen to use .06 / .94 based on pretty bad train AUC all around and better validation AUC\n",
    "# unclear why this cell shows as not having been run despite having output\n",
    "tf.keras.backend.clear_session()\n",
    "nn_model.set_weights(nn_weights)\n",
    "nn_model = keras.Model(inputs=[audio_input, video_input],outputs=[nn_output])\n",
    "nn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[keras.metrics.AUC()])\n",
    "nn_model.fit(x=[train_audio, train_video], y=y_train_v, class_weight={0: .06, 1: 0.94},validation_data=([val_audio,val_video], y_val_v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2f1a0ed2-4849-4f19-b762-daf97652858e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 6s 286ms/step\n",
      "19/19 [==============================] - 6s 289ms/step - loss: 0.7406 - auc: 0.6059\n",
      "Classifier: NN\n",
      "Class 0 Precision:  0.9156626506024096  |  Class 1 Precision: 0.0\n",
      "Class 0 Recall:     1.0  |  Class 1 Recall:    0.0\n",
      "Class 0 FScore:     0.9559748427672956  |  Class 1 FScore:    0.0\n",
      "Class 0 Support:    532  |  Class 1 Support:   49\n",
      "NN Accuracy Score: 0.9156626506024096\n",
      "NN ROC/AUC Score:  0.6059153079986572\n",
      "[[532   0]\n",
      " [ 49   0]]\n"
     ]
    }
   ],
   "source": [
    "nn_raw_preds = nn_model.predict([test_audio, test_video])\n",
    "nn_preds = []\n",
    "for probs in nn_raw_preds:\n",
    "    nn_preds.append(np.argmax(probs))\n",
    "precision, recall, fscore, support = precision_recall_fscore_support(y_test_v, nn_preds)\n",
    "accuracy = accuracy_score(y_test_v, nn_preds)\n",
    "nn_loss, nn_auc = nn_model.evaluate([test_audio,test_video], y_test_v)\n",
    "print(\"Classifier: NN\")\n",
    "print(\"Class 0 Precision:  {}  |  Class 1 Precision: {}\".format(precision[0], precision[1]))\n",
    "print(\"Class 0 Recall:     {}  |  Class 1 Recall:    {}\".format(recall[0], recall[1]))\n",
    "print(\"Class 0 FScore:     {}  |  Class 1 FScore:    {}\".format(fscore[0], fscore[1]))\n",
    "print(\"Class 0 Support:    {}  |  Class 1 Support:   {}\".format(support[0], support[1]))\n",
    "print(\"NN Accuracy Score: {}\".format(accuracy))\n",
    "print(\"NN ROC/AUC Score:  {}\".format(nn_auc))\n",
    "print(confusion_matrix(y_test_v,nn_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bb0da8-6ee2-41d8-9c07-bbce857a3136",
   "metadata": {},
   "source": [
    "### Testing Class Weights with Early Stopping (Long Run Time Warning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c689d84f-eee0-40f5-a762-0d0c392d1f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01 0.99\n",
      "88/88 [==============================] - 464s 5s/step - loss: 0.4465 - auc_1: 0.5408 - val_loss: 1.8551 - val_auc_1: 0.5548\n",
      "0.02 0.98\n",
      "88/88 [==============================] - 452s 5s/step - loss: 0.8815 - auc_2: 0.4891 - val_loss: 1.1558 - val_auc_2: 0.5686\n",
      "0.03 0.97\n",
      "88/88 [==============================] - 432s 5s/step - loss: 1.1165 - auc_3: 0.5053 - val_loss: 1.2300 - val_auc_3: 0.4893\n",
      "0.04 0.96\n",
      "88/88 [==============================] - 454s 5s/step - loss: 1.3444 - auc_4: 0.5271 - val_loss: 0.8515 - val_auc_4: 0.4372\n",
      "0.05 0.95\n",
      "88/88 [==============================] - 458s 5s/step - loss: 1.3962 - auc_5: 0.4949 - val_loss: 0.7457 - val_auc_5: 0.4928\n",
      "0.06 0.94\n",
      "88/88 [==============================] - 456s 5s/step - loss: 1.1953 - auc_6: 0.5323 - val_loss: 0.7022 - val_auc_6: 0.5677\n",
      "0.07 0.9299999999999999\n",
      "88/88 [==============================] - 446s 5s/step - loss: 2.1401 - auc_7: 0.5040 - val_loss: 0.6966 - val_auc_7: 0.5709\n",
      "0.08 0.92\n",
      "88/88 [==============================] - 449s 5s/step - loss: 2.4804 - auc_8: 0.4697 - val_loss: 0.7156 - val_auc_8: 0.5665\n",
      "0.09 0.91\n",
      "88/88 [==============================] - 464s 5s/step - loss: 3.1589 - auc_9: 0.4676 - val_loss: 0.6758 - val_auc_9: 0.4864\n",
      "0.1 0.9\n",
      "88/88 [==============================] - 458s 5s/step - loss: 1.2110 - auc_10: 0.5149 - val_loss: 0.6731 - val_auc_10: 0.6301\n"
     ]
    }
   ],
   "source": [
    "# early stopping implementation\n",
    "for val in weight_vals:\n",
    "    w_0 = val\n",
    "    w_1 = 1 - w_0\n",
    "    print(w_0, w_1)\n",
    "    nn_model.set_weights(nn_weights)\n",
    "    nn_model = keras.Model(inputs=[audio_input, video_input],outputs=[nn_output])\n",
    "    nn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[keras.metrics.AUC()])\n",
    "    nn_model.fit(x=[train_audio, train_video], y=y_train_v, class_weight={0: w_0, 1: w_1},validation_data=([val_audio,val_video], y_val_v), callbacks=[callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f055ce3-a462-40fd-a1e2-a25d1956baf6",
   "metadata": {},
   "source": [
    "#### Run the best model above and generate some metrics\n",
    "##### You will need to do some analysis on the runs above and manually adjust the class_weight parameter below. This is due to a combination of random initialization behaviors between notebook runs (TensorFlow will change the values everytime the kernel restarts) and a judgement call between training and validation performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0f8fbe72-b804-4b71-9c75-a3a3507480b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 453s 5s/step - loss: 1.4899 - auc_10: 0.5084 - val_loss: 0.6838 - val_auc_10: 0.5149\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14a2c54aa7c0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best performance was .1 / .9\n",
    "nn_model.set_weights(nn_weights)\n",
    "nn_model.fit(x=[train_audio, train_video], y=y_train_v, class_weight={0: .1, 1: .9},validation_data=([val_audio,val_video], y_val_v), callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b957051-74ed-49ac-afe2-00d03e87fcc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 5s 278ms/step\n",
      "19/19 [==============================] - 5s 282ms/step - loss: 0.6832 - auc_10: 0.5102\n",
      "Classifier: NN\n",
      "Class 0 Precision:  0.9156626506024096  |  Class 1 Precision: 0.0\n",
      "Class 0 Recall:     1.0  |  Class 1 Recall:    0.0\n",
      "Class 0 FScore:     0.9559748427672956  |  Class 1 FScore:    0.0\n",
      "Class 0 Support:    532  |  Class 1 Support:   49\n",
      "NN Accuracy Score: 0.9156626506024096\n",
      "NN ROC/AUC Score:  0.5102232694625854\n",
      "[[532   0]\n",
      " [ 49   0]]\n"
     ]
    }
   ],
   "source": [
    "nn_raw_preds = nn_model.predict([test_audio, test_video])\n",
    "nn_preds = []\n",
    "for probs in nn_raw_preds:\n",
    "    nn_preds.append(np.argmax(probs))\n",
    "precision, recall, fscore, support = precision_recall_fscore_support(y_test_v, nn_preds)\n",
    "accuracy = accuracy_score(y_test_v, nn_preds)\n",
    "nn_loss, nn_auc = nn_model.evaluate([test_audio,test_video], y_test_v)\n",
    "print(\"Classifier: NN\")\n",
    "print(\"Class 0 Precision:  {}  |  Class 1 Precision: {}\".format(precision[0], precision[1]))\n",
    "print(\"Class 0 Recall:     {}  |  Class 1 Recall:    {}\".format(recall[0], recall[1]))\n",
    "print(\"Class 0 FScore:     {}  |  Class 1 FScore:    {}\".format(fscore[0], fscore[1]))\n",
    "print(\"Class 0 Support:    {}  |  Class 1 Support:   {}\".format(support[0], support[1]))\n",
    "print(\"NN Accuracy Score: {}\".format(accuracy))\n",
    "print(\"NN ROC/AUC Score:  {}\".format(nn_auc))\n",
    "print(confusion_matrix(y_test_v,nn_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2473778f-77d9-4d75-8441-2511c26a1104",
   "metadata": {},
   "source": [
    "### Testing Epochs (Very Long Run Time Warning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a3300cbc-e9a6-4a22-9ce0-5c290e231f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "88/88 [==============================] - 432s 5s/step - loss: 2.6756 - auc_14: 0.4881 - val_loss: 0.6830 - val_auc_14: 0.5175\n",
      "Epoch 2/2\n",
      "88/88 [==============================] - 433s 5s/step - loss: 0.1245 - auc_14: 0.5192 - val_loss: 1.1615 - val_auc_14: 0.4838\n",
      "Epoch 1/3\n",
      "88/88 [==============================] - 433s 5s/step - loss: 2.2498 - auc_15: 0.4903 - val_loss: 0.5089 - val_auc_15: 0.5631\n",
      "Epoch 2/3\n",
      "88/88 [==============================] - 432s 5s/step - loss: 0.1331 - auc_15: 0.5337 - val_loss: 0.9021 - val_auc_15: 0.5167\n",
      "Epoch 3/3\n",
      "88/88 [==============================] - 433s 5s/step - loss: 0.1225 - auc_15: 0.5088 - val_loss: 0.6831 - val_auc_15: 0.5278\n",
      "Epoch 1/4\n",
      "88/88 [==============================] - 439s 5s/step - loss: 5.9054 - auc_16: 0.5232 - val_loss: 0.6512 - val_auc_16: 0.6237\n",
      "Epoch 2/4\n",
      "88/88 [==============================] - 434s 5s/step - loss: 0.1230 - auc_16: 0.5502 - val_loss: 0.4584 - val_auc_16: 0.6019\n",
      "Epoch 3/4\n",
      "88/88 [==============================] - 434s 5s/step - loss: 0.1196 - auc_16: 0.5916 - val_loss: 0.6751 - val_auc_16: 0.6096\n",
      "Epoch 4/4\n",
      "88/88 [==============================] - 434s 5s/step - loss: 0.1370 - auc_16: 0.5928 - val_loss: 0.5401 - val_auc_16: 0.5603\n"
     ]
    }
   ],
   "source": [
    "# you can change class weights based on previous results\n",
    "epoch_list = [2,3,4]\n",
    "for epoch in epoch_list:\n",
    "    nn_model.set_weights(nn_weights)\n",
    "    nn_model = keras.Model(inputs=[audio_input, video_input],outputs=[nn_output])\n",
    "    nn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[keras.metrics.AUC()])\n",
    "    nn_model.fit(x=[train_audio, train_video], y=y_train_v, class_weight={0:.1, 1:.9},validation_data=([val_audio,val_video], y_val_v), \n",
    "                 callbacks=[callback],epochs=epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbd6516-0717-449e-9141-f465faa3470f",
   "metadata": {},
   "source": [
    "#### Run the best model above and generate some metrics\n",
    "##### You will need to do some analysis on the runs above and manually adjust the class_weight parameter below. This is due to a combination of random initialization behaviors between notebook runs (TensorFlow will change the values everytime the kernel restarts) and a judgement call between training and validation performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3cbe05b8-f6a5-4f1a-9edb-a639f15c778d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "88/88 [==============================] - 435s 5s/step - loss: 3.7677 - auc_16: 0.4826 - val_loss: 0.8308 - val_auc_16: 0.4214\n",
      "Epoch 2/4\n",
      "88/88 [==============================] - 433s 5s/step - loss: 0.0914 - auc_16: 0.5634 - val_loss: 0.8184 - val_auc_16: 0.5630\n",
      "Epoch 3/4\n",
      "88/88 [==============================] - 433s 5s/step - loss: 0.0876 - auc_16: 0.6031 - val_loss: 0.7729 - val_auc_16: 0.5738\n",
      "Epoch 4/4\n",
      "88/88 [==============================] - 434s 5s/step - loss: 0.0830 - auc_16: 0.6542 - val_loss: 0.7958 - val_auc_16: 0.5968\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14a2bff1f730>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adjust the epochs\n",
    "nn_model.set_weights(nn_weights)\n",
    "nn_model.fit(x=[train_audio, train_video], y=y_train_v, class_weight={0: .06, 1: .94},validation_data=([val_audio,val_video], y_val_v), \n",
    "             callbacks=[callback], epochs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "59647901-f1dc-4cbf-af9f-17d16ec2365a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 6s 290ms/step\n",
      "19/19 [==============================] - 6s 298ms/step - loss: 0.8061 - auc_16: 0.5619\n",
      "Classifier: NN\n",
      "Class 0 Precision:  0.9156626506024096  |  Class 1 Precision: 0.0\n",
      "Class 0 Recall:     1.0  |  Class 1 Recall:    0.0\n",
      "Class 0 FScore:     0.9559748427672956  |  Class 1 FScore:    0.0\n",
      "Class 0 Support:    532  |  Class 1 Support:   49\n",
      "NN Accuracy Score: 0.9156626506024096\n",
      "NN ROC/AUC Score:  0.5619149804115295\n",
      "[[532   0]\n",
      " [ 49   0]]\n"
     ]
    }
   ],
   "source": [
    "nn_raw_preds = nn_model.predict([test_audio, test_video])\n",
    "nn_preds = []\n",
    "for probs in nn_raw_preds:\n",
    "    nn_preds.append(np.argmax(probs))\n",
    "precision, recall, fscore, support = precision_recall_fscore_support(y_test_v, nn_preds)\n",
    "accuracy = accuracy_score(y_test_v, nn_preds)\n",
    "nn_loss, nn_auc = nn_model.evaluate([test_audio,test_video], y_test_v)\n",
    "print(\"Classifier: NN\")\n",
    "print(\"Class 0 Precision:  {}  |  Class 1 Precision: {}\".format(precision[0], precision[1]))\n",
    "print(\"Class 0 Recall:     {}  |  Class 1 Recall:    {}\".format(recall[0], recall[1]))\n",
    "print(\"Class 0 FScore:     {}  |  Class 1 FScore:    {}\".format(fscore[0], fscore[1]))\n",
    "print(\"Class 0 Support:    {}  |  Class 1 Support:   {}\".format(support[0], support[1]))\n",
    "print(\"NN Accuracy Score: {}\".format(accuracy))\n",
    "print(\"NN ROC/AUC Score:  {}\".format(nn_auc))\n",
    "print(confusion_matrix(y_test_v,nn_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edc5956-271a-4f3a-9174-71ea4f54110a",
   "metadata": {},
   "source": [
    "### \"Two Stream\" LSTM - Frame Data\n",
    "#### Network Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1d312412-02cf-42db-b1bd-eed6198f6030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Roberto Chavez's paper\n",
    "# https://github.com/rchavezj/Label_YT_Videos\n",
    "\n",
    "stream_x1 = keras.Input(shape=(100,128))\n",
    "stream_x2 = keras.Input(shape=(100,1024))\n",
    "\n",
    "stream_fc_1_x1 = keras.layers.Dense(512, activation='relu')(stream_x1) \n",
    "stream_fc_1_x2 = keras.layers.Dense(512, activation='relu')(stream_x2) \n",
    "\n",
    "# LSTM\n",
    "stream_lstm_1_x1 = keras.layers.LSTM(128, return_sequences=True, go_backwards=False)(stream_fc_1_x1)\n",
    "stream_lstm_1_x2 = keras.layers.LSTM(1024, return_sequences=True, go_backwards=False)(stream_fc_1_x2)\n",
    "\n",
    "# LSTM\n",
    "stream_lstm_2_x1 = keras.layers.LSTM(128, return_sequences=True, go_backwards=True)(stream_lstm_1_x1)\n",
    "stream_lstm_2_x2 = keras.layers.LSTM(1024, return_sequences=True, go_backwards=True)(stream_lstm_1_x2)\n",
    "\n",
    "stream_dropout_1_x1 = keras.layers.Dropout(rate=0.5)(stream_lstm_2_x1)\n",
    "stream_dropout_1_x2 = keras.layers.Dropout(rate=0.5)(stream_lstm_2_x2)\n",
    "\n",
    "stream_fc_2_x1 = keras.layers.Dense(1, activation='relu')(stream_dropout_1_x1) \n",
    "stream_fc_2_x2 = keras.layers.Dense(1, activation='relu')(stream_dropout_1_x2) \n",
    "\n",
    "stream_fc_3_x1 = keras.layers.Dense(16, activation='relu')(stream_dropout_1_x1) \n",
    "stream_fc_3_x2 = keras.layers.Dense(16, activation='relu')(stream_dropout_1_x2)\n",
    "\n",
    "stream_pool_1_x1 = keras.layers.GlobalMaxPooling1D()(stream_fc_3_x1)\n",
    "stream_pool_1_x2 = keras.layers.GlobalMaxPooling1D()(stream_fc_3_x2)\n",
    "\n",
    "stream_merge_1 = keras.layers.concatenate([stream_pool_1_x1, stream_pool_1_x2])\n",
    "\n",
    "stream_fc_2 = keras.layers.Dense(8192, activation='relu')(stream_merge_1) \n",
    "\n",
    "stream_fc_3 = keras.layers.Dense(4096, activation='relu')(stream_fc_2) \n",
    "\n",
    "stream_output = keras.layers.Dense(1, activation='sigmoid')(stream_fc_3)\n",
    "\n",
    "# # Complete Model Diagram\n",
    "stream_lstm_model = keras.Model(inputs=[stream_x1, stream_x2],outputs=[stream_output])\n",
    "stream_lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[keras.metrics.AUC()] )\n",
    "\n",
    "stream_weights = stream_lstm_model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dc431d-b585-456c-a826-ba9f445197bd",
   "metadata": {},
   "source": [
    "### Testing Class Weights (Long Run Time Warning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b19397f4-645a-4ee5-981d-a7d35de8bb80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01 0.99\n",
      "88/88 [==============================] - 1240s 14s/step - loss: 0.0332 - auc_18: 0.4993 - val_loss: 1.1362 - val_auc_18: 0.5227\n",
      "0.02 0.98\n",
      "88/88 [==============================] - 1445s 16s/step - loss: 0.0526 - auc_18: 0.4683 - val_loss: 1.0762 - val_auc_18: 0.5000\n",
      "0.03 0.97\n",
      "88/88 [==============================] - 1483s 17s/step - loss: 0.0646 - auc_18: 0.5108 - val_loss: 0.8119 - val_auc_18: 0.5000\n",
      "0.04 0.96\n",
      "88/88 [==============================] - 1281s 14s/step - loss: 0.0774 - auc_18: 0.4880 - val_loss: 0.7551 - val_auc_18: 0.5000\n",
      "0.05 0.95\n",
      "88/88 [==============================] - 1334s 15s/step - loss: 0.0860 - auc_18: 0.4964 - val_loss: 0.8478 - val_auc_18: 0.5000\n",
      "0.06 0.94\n",
      "88/88 [==============================] - 1410s 16s/step - loss: 0.0941 - auc_18: 0.4693 - val_loss: 0.7088 - val_auc_18: 0.5000\n",
      "0.07 0.9299999999999999\n",
      "88/88 [==============================] - 1310s 15s/step - loss: 0.1023 - auc_18: 0.4809 - val_loss: 0.7021 - val_auc_18: 0.5000\n",
      "0.08 0.92\n",
      "88/88 [==============================] - 1349s 15s/step - loss: 0.1084 - auc_18: 0.4625 - val_loss: 0.7043 - val_auc_18: 0.5000\n",
      "0.09 0.91\n",
      "88/88 [==============================] - 1275s 14s/step - loss: 0.1137 - auc_18: 0.4971 - val_loss: 0.6870 - val_auc_18: 0.5000\n",
      "0.1 0.9\n",
      "88/88 [==============================] - 1382s 16s/step - loss: 0.1193 - auc_18: 0.5223 - val_loss: 0.6849 - val_auc_18: 0.5000\n"
     ]
    }
   ],
   "source": [
    "for val in weight_vals:\n",
    "    w_0 = val\n",
    "    w_1 = 1 - w_0\n",
    "    print(w_0, w_1)\n",
    "    stream_lstm_model.set_weights(stream_weights)\n",
    "    stream_lstm_model.fit(x=[audio_train_frames, rgb_train_frames], y=y_train_v,class_weight={0:w_0,1:w_1}, \n",
    "                          validation_data=([audio_val_frames, rgb_val_frames], y_val_v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7248a75f-ad78-4d97-911c-c563f8121572",
   "metadata": {},
   "source": [
    "#### Run the best model above and generate some metrics\n",
    "##### You will need to do some analysis on the runs above and manually adjust the class_weight parameter below. This is due to a combination of random initialization behaviors between notebook runs (TensorFlow will change the values everytime the kernel restarts) and a judgement call between training and validation performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b45714b4-a052-4b10-b39e-fe81eca2f1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 1295s 15s/step - loss: 0.0316 - auc_18: 0.5149 - val_loss: 0.8544 - val_auc_18: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14a2c18cb820>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in the previous run the AUC scores did a worse job than normal, so we went with the only value that produced a val AUC that wasn't .50\n",
    "stream_lstm_model.set_weights(stream_weights)\n",
    "\n",
    "stream_lstm_model.fit(x=[audio_train_frames, rgb_train_frames], y=y_train_v, class_weight={0:.01,1:.99}, validation_data=([audio_val_frames, rgb_val_frames], y_val_v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2552ecd1-ca68-4134-8505-978b948af4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 39s 2s/step\n",
      "19/19 [==============================] - 38s 2s/step - loss: 0.8547 - auc_18: 0.5000\n",
      "Classifier: LSTM\n",
      "Class 0 Precision:  0.9156626506024096  |  Class 1 Precision: 0.0\n",
      "Class 0 Recall:     1.0  |  Class 1 Recall:    0.0\n",
      "Class 0 FScore:     0.9559748427672956  |  Class 1 FScore:    0.0\n",
      "Class 0 Support:    532  |  Class 1 Support:   49\n",
      "LSTM Accuracy Score: 0.9156626506024096\n",
      "LSTM ROC/AUC Score:  0.5\n",
      "[[532   0]\n",
      " [ 49   0]]\n"
     ]
    }
   ],
   "source": [
    "lstm_raw_preds = stream_lstm_model.predict([audio_test_frames, rgb_test_frames])\n",
    "lstm_preds = []\n",
    "for probs in lstm_raw_preds:\n",
    "    lstm_preds.append(np.argmax(probs))\n",
    "precision, recall, fscore, support = precision_recall_fscore_support(y_test_v, lstm_preds)\n",
    "accuracy = accuracy_score(y_test_v, lstm_preds)\n",
    "lstm_loss, lstm_auc = stream_lstm_model.evaluate([audio_test_frames, rgb_test_frames], y_test_v)\n",
    "print(\"Classifier: LSTM\")\n",
    "print(\"Class 0 Precision:  {}  |  Class 1 Precision: {}\".format(precision[0], precision[1]))\n",
    "print(\"Class 0 Recall:     {}  |  Class 1 Recall:    {}\".format(recall[0], recall[1]))\n",
    "print(\"Class 0 FScore:     {}  |  Class 1 FScore:    {}\".format(fscore[0], fscore[1]))\n",
    "print(\"Class 0 Support:    {}  |  Class 1 Support:   {}\".format(support[0], support[1]))\n",
    "print(\"LSTM Accuracy Score: {}\".format(accuracy))\n",
    "print(\"LSTM ROC/AUC Score:  {}\".format(lstm_auc))\n",
    "print(confusion_matrix(y_test_v,lstm_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2626c4-362a-4d64-80e3-409efea34019",
   "metadata": {},
   "source": [
    "### Testing Class Weights with Early Stopping (Long Run Time Warning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6555603a-7b49-424d-8716-b8ec48695a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01 0.99\n",
      "88/88 [==============================] - 1365s 15s/step - loss: 0.0323 - auc_18: 0.5158 - val_loss: 0.8283 - val_auc_18: 0.5000\n",
      "0.02 0.98\n",
      "88/88 [==============================] - 1217s 14s/step - loss: 0.0529 - auc_18: 0.4991 - val_loss: 0.8251 - val_auc_18: 0.4934\n",
      "0.03 0.97\n",
      "88/88 [==============================] - 1365s 15s/step - loss: 0.0657 - auc_18: 0.5059 - val_loss: 0.8135 - val_auc_18: 0.5000\n",
      "0.04 0.96\n",
      "88/88 [==============================] - 1292s 15s/step - loss: 0.0794 - auc_18: 0.4947 - val_loss: 0.7571 - val_auc_18: 0.5000\n",
      "0.05 0.95\n",
      "88/88 [==============================] - 1368s 15s/step - loss: 0.0871 - auc_18: 0.4762 - val_loss: 0.7545 - val_auc_18: 0.5000\n",
      "0.06 0.94\n",
      "88/88 [==============================] - 1298s 15s/step - loss: 0.0942 - auc_18: 0.4891 - val_loss: 0.7749 - val_auc_18: 0.5000\n",
      "0.07 0.9299999999999999\n",
      "88/88 [==============================] - 1377s 16s/step - loss: 0.1020 - auc_18: 0.5098 - val_loss: 0.7413 - val_auc_18: 0.5000\n",
      "0.08 0.92\n",
      "88/88 [==============================] - 1297s 15s/step - loss: 0.1080 - auc_18: 0.5043 - val_loss: 0.6953 - val_auc_18: 0.5000\n",
      "0.09 0.91\n",
      "88/88 [==============================] - 1479s 17s/step - loss: 0.1140 - auc_18: 0.4661 - val_loss: 0.6842 - val_auc_18: 0.5000\n",
      "0.1 0.9\n",
      "88/88 [==============================] - 1315s 15s/step - loss: 0.1190 - auc_18: 0.4738 - val_loss: 0.6897 - val_auc_18: 0.5000\n"
     ]
    }
   ],
   "source": [
    "for val in weight_vals:\n",
    "    w_0 = val\n",
    "    w_1 = 1 - w_0\n",
    "    print(w_0, w_1)\n",
    "    stream_lstm_model.set_weights(stream_weights)\n",
    "    stream_lstm_model.fit(x=[audio_train_frames, rgb_train_frames], y=y_train_v,class_weight={0:w_0,1:w_1}, \n",
    "                          validation_data=([audio_val_frames, rgb_val_frames], y_val_v), callbacks=[callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607965cd-3f0b-4bf1-9b6a-37cfcd918bcd",
   "metadata": {},
   "source": [
    "#### Run the best model above and generate some metrics\n",
    "##### You will need to do some analysis on the runs above and manually adjust the class_weight parameter below. This is due to a combination of random initialization behaviors between notebook runs (TensorFlow will change the values everytime the kernel restarts) and a judgement call between training and validation performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "df14e75c-1686-45ce-9fc1-be419d2c28df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 1492s 17s/step - loss: 0.1017 - auc_18: 0.4692 - val_loss: 0.7104 - val_auc_18: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14a2b57afdc0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in this case we are going with the highest training AUC score since all but 1 validationg scores are the same\n",
    "stream_lstm_model.set_weights(stream_weights)\n",
    "\n",
    "stream_lstm_model.fit(x=[audio_train_frames, rgb_train_frames], y=y_train_v, class_weight={0:.07,1:.93}, \n",
    "                      validation_data=([audio_val_frames, rgb_val_frames], y_val_v), callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e34b18ea-588a-46f1-bc46-973eb7df1547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 41s 2s/step\n",
      "19/19 [==============================] - 39s 2s/step - loss: 0.7104 - auc_18: 0.5000\n",
      "Classifier: LSTM\n",
      "Class 0 Precision:  0.9156626506024096  |  Class 1 Precision: 0.0\n",
      "Class 0 Recall:     1.0  |  Class 1 Recall:    0.0\n",
      "Class 0 FScore:     0.9559748427672956  |  Class 1 FScore:    0.0\n",
      "Class 0 Support:    532  |  Class 1 Support:   49\n",
      "LSTM Accuracy Score: 0.9156626506024096\n",
      "LSTM ROC/AUC Score:  0.5\n",
      "[[532   0]\n",
      " [ 49   0]]\n"
     ]
    }
   ],
   "source": [
    "lstm_raw_preds = stream_lstm_model.predict([audio_test_frames, rgb_test_frames])\n",
    "lstm_preds = []\n",
    "for probs in lstm_raw_preds:\n",
    "    lstm_preds.append(np.argmax(probs))\n",
    "precision, recall, fscore, support = precision_recall_fscore_support(y_test_v, lstm_preds)\n",
    "accuracy = accuracy_score(y_test_v, lstm_preds)\n",
    "lstm_loss, lstm_auc = stream_lstm_model.evaluate([audio_test_frames, rgb_test_frames], y_test_v)\n",
    "print(\"Classifier: LSTM\")\n",
    "print(\"Class 0 Precision:  {}  |  Class 1 Precision: {}\".format(precision[0], precision[1]))\n",
    "print(\"Class 0 Recall:     {}  |  Class 1 Recall:    {}\".format(recall[0], recall[1]))\n",
    "print(\"Class 0 FScore:     {}  |  Class 1 FScore:    {}\".format(fscore[0], fscore[1]))\n",
    "print(\"Class 0 Support:    {}  |  Class 1 Support:   {}\".format(support[0], support[1]))\n",
    "print(\"LSTM Accuracy Score: {}\".format(accuracy))\n",
    "print(\"LSTM ROC/AUC Score:  {}\".format(lstm_auc))\n",
    "print(confusion_matrix(y_test_v,lstm_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b971ba03-d76f-4b38-9e7a-25ef347eb2aa",
   "metadata": {},
   "source": [
    "### Testing Epochs (VERY Long Run Time Warning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "95ac7fa3-65c5-4c7d-aa45-17444117f67a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "88/88 [==============================] - 1533s 17s/step - loss: 0.1192 - auc_18: 0.4822 - val_loss: 0.6797 - val_auc_18: 0.5000\n",
      "Epoch 2/2\n",
      "88/88 [==============================] - 1536s 17s/step - loss: 0.1156 - auc_18: 0.4790 - val_loss: 0.6542 - val_auc_18: 0.5000\n",
      "Epoch 1/3\n",
      "88/88 [==============================] - 1384s 16s/step - loss: 0.1205 - auc_18: 0.5300 - val_loss: 0.6898 - val_auc_18: 0.5000\n",
      "Epoch 2/3\n",
      "88/88 [==============================] - 1370s 16s/step - loss: 0.1154 - auc_18: 0.5155 - val_loss: 0.6819 - val_auc_18: 0.5000\n",
      "Epoch 3/3\n",
      "88/88 [==============================] - 1396s 16s/step - loss: 0.1155 - auc_18: 0.5070 - val_loss: 0.6607 - val_auc_18: 0.5000\n"
     ]
    }
   ],
   "source": [
    "epoch_list = [2,3]\n",
    "for epoch in epoch_list:\n",
    "    stream_lstm_model.set_weights(stream_weights)\n",
    "    stream_lstm_0model.fit(x=[audio_train_frames, rgb_train_frames], y=y_train_v,class_weight={0:w_0,1:w_1}, \n",
    "                          validation_data=([audio_val_frames, rgb_val_frames], y_val_v), callbacks=[callback], epochs=epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe5f49c-c891-4865-ac9d-f2f6b5213585",
   "metadata": {},
   "source": [
    "#### Run the best model above and generate some metrics\n",
    "##### In this case the performance never seems to improve, so rather than retraining we will just print out the metrics from the last iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0fe6b3f0-616c-4d13-8bd8-16b894dcbc15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 39s 2s/step\n",
      "19/19 [==============================] - 39s 2s/step - loss: 0.6606 - auc_18: 0.5000\n",
      "Classifier: LSTM\n",
      "Class 0 Precision:  0.9156626506024096  |  Class 1 Precision: 0.0\n",
      "Class 0 Recall:     1.0  |  Class 1 Recall:    0.0\n",
      "Class 0 FScore:     0.9559748427672956  |  Class 1 FScore:    0.0\n",
      "Class 0 Support:    532  |  Class 1 Support:   49\n",
      "LSTM Accuracy Score: 0.9156626506024096\n",
      "LSTM ROC/AUC Score:  0.5\n",
      "[[532   0]\n",
      " [ 49   0]]\n"
     ]
    }
   ],
   "source": [
    "lstm_raw_preds = stream_lstm_model.predict([audio_test_frames, rgb_test_frames])\n",
    "lstm_preds = []\n",
    "for probs in lstm_raw_preds:\n",
    "    lstm_preds.append(np.argmax(probs))\n",
    "precision, recall, fscore, support = precision_recall_fscore_support(y_test_v, lstm_preds)\n",
    "accuracy = accuracy_score(y_test_v, lstm_preds)\n",
    "lstm_loss, lstm_auc = stream_lstm_model.evaluate([audio_test_frames, rgb_test_frames], y_test_v)\n",
    "print(\"Classifier: LSTM\")\n",
    "print(\"Class 0 Precision:  {}  |  Class 1 Precision: {}\".format(precision[0], precision[1]))\n",
    "print(\"Class 0 Recall:     {}  |  Class 1 Recall:    {}\".format(recall[0], recall[1]))\n",
    "print(\"Class 0 FScore:     {}  |  Class 1 FScore:    {}\".format(fscore[0], fscore[1]))\n",
    "print(\"Class 0 Support:    {}  |  Class 1 Support:   {}\".format(support[0], support[1]))\n",
    "print(\"LSTM Accuracy Score: {}\".format(accuracy))\n",
    "print(\"LSTM ROC/AUC Score:  {}\".format(lstm_auc))\n",
    "print(confusion_matrix(y_test_v,lstm_preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
