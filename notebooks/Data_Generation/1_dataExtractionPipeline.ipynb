{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adfb66ee-5b03-4550-84cb-56f9f77d9480",
   "metadata": {},
   "source": [
    "## Data Extraction Pipeline\n",
    "\n",
    "#### This notebook serves to automate the process of collecting the data from the YouTube API and then performing the translation process on them. <span style=\"color:red\">It is only recommended for use by those who are uncomfortable manually implementing the scripting steps from the command line, as those will provide significantly more flexibility. </span><br>\n",
    "#### Users MUST fill in 5 pieces of information for this to work:\n",
    "* The location of the cloned repository\n",
    "* The location of the downloaded dataset\n",
    "* The path to the first api key (single line text document)\n",
    "* The path to the second api key (single ine text document)\n",
    "* Whether to do a fresh lookup / translation. Set to True if fresh data is desired, leave as False to reuse existing data\n",
    "\n",
    "#### By default all output will be placed in a folder called \"intermediate\"  at the SAME level as the repository which you can then point the BertTopics portion of the pipeline to. This is to aid in the ease of cleanup after you have finished doing topic modeling, you can simply delete the intermediate folder. You can change this behavior by adjusting the relevant portions of the magic commands below, however it is recommended that any user comfortable with that approach simply uses the command line approach instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c78146a-96be-4160-81bd-7c28d351792d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59075da1-1af8-48be-927c-c6aa40445433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REQUIREMENT 1: path to where this repo exists\n",
    "repo_location = \"/nfs/turbo/seas-nhcarter/human_wildlife_interactions/repo/\"\n",
    "# REQUIREMENT 2: path to where the downloaded dataset is stored\n",
    "storage_path = \"/nfs/turbo/seas-nhcarter/human_wildlife_interactions\"\n",
    "\n",
    "# REQUIREMENT 3 / 4: api key file_path (expects a 1 line .txt file)\n",
    "# (it is recommended that you use 2 different keys because each can only handle 10k lookups per day and each video requires 2 lookups,\n",
    "# however if you choose to use 1 please enter it twice)\n",
    "api_key1_path = \"/nfs/turbo/seas-nhcarter/human_wildlife_interactions/repo/human-wildlife-interactions/pipeline/youtubeapi1.txt\"\n",
    "api_key2_path = \"/nfs/turbo/seas-nhcarter/human_wildlife_interactions/repo/human-wildlife-interactions/pipeline/youtubeapi2.txt\"\n",
    "\n",
    "# REQUIREMENT 5: do you want a new API lookup? (set to false to use existing data)\n",
    "fresh_data = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86111f5e-d1a0-49c6-98f9-b851b6270aa8",
   "metadata": {},
   "source": [
    "API Query Settings (Optional, currently configured to our Wildlife lookups):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32d80bb0-a707-483c-a9af-e6e21897dc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# desired topic (must be from the available yt8m topic list: default is 'Wildlife')\n",
    "yt8m_topic = \"Wildlife\"\n",
    "# genre topic belongs to (as defined here: https://research.google.com/youtube8m/csv/2/vocabulary.csv)\n",
    "yt8m_genre = \"Pets & Animals\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a7338f-3816-435f-95af-313bb5a044b8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports / Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c068676d-1dee-4e3a-b069-27fa49a7215c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general utility imports\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "# local utility imports\n",
    "import pipeline_utility\n",
    "# data manipulation imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1777d182-0d46-401a-8f6b-90cc90943bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.2\n",
      "1.22.4\n",
      "Python 3.9.7\n"
     ]
    }
   ],
   "source": [
    "# show dependencies\n",
    "print(pd.__version__)\n",
    "print(np.__version__)\n",
    "! python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72981cf7-358e-46b9-8479-2743ae442f4d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'shutil' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1329863/2811989284.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mvideo_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage_path\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"video/train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mframe_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage_path\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"frame/train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'shutil' is not defined"
     ]
    }
   ],
   "source": [
    "# path to the yt8m video training data\n",
    "video_training_location = \"/nfs/turbo/seas-nhcarter/human_wildlife_interactions/video/train\"\n",
    "# path to the yt8m frame training data\n",
    "frame_training_location = \"/nfs/turbo/seas-nhcarter/human_wildlife_interactions/frame/train\"\n",
    "\n",
    "repo_path = Path(repo_location)\n",
    "api1_path = Path(api_key1_path)\n",
    "api2_path = Path(api_key2_path)\n",
    "storage_path = Path(storage_path)\n",
    "video_path = Path(storage_path / \"video/train\")\n",
    "frame_path = Path(storage_path / \"frame/train\")\n",
    "shutil.copyfile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68752c4-cf89-481b-aae4-f338c8c0eef9",
   "metadata": {},
   "source": [
    "## Get the Relevant Video IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eaa17e9-a58a-4ee3-ab4e-488d5bc83bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of all relevant video ids\n",
    "yt8m_ids = pipeline_utility.get_video_ids(yt8m_genre, yt8m_topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aba9f09-e755-464a-8993-91b28aaffcd1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Get the Video Description and Comment information from the YouTube API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df90d53-bf01-4bd6-92f0-1ed1fc0472e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set fresh_data to True to do a new API pull (recommend not running more than once per day due to API data limits)\n",
    "if fresh_data:    \n",
    "    # check if intermediate directory exists, otherwise make it\n",
    "    working_path = os.path.join(repo_location, \"intermediate/api_data\")\n",
    "    if not os.path.isdir(working_path):\n",
    "        os.makedirs(working_path)\n",
    "\n",
    "    # grab the video titles / descriptions and comments and then write them to files for follow on steps\n",
    "    # this step generally takes about 30 - 40 minutes depending on your connection / setup\n",
    "    api1 = pipeline_utility.api_key(api_key1_path)\n",
    "    api2 = pipeline_utility.api_key(api_key2_path)\n",
    "    pipeline_utility.lookup_videos(yt8m_ids, working_path, api1, api2)\n",
    "else:\n",
    "    print(\"Using existing data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9d781f-dc18-4d72-9b3d-d8fdc771efdf",
   "metadata": {},
   "source": [
    "## Send to Translation and Topic Modeling Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99004e9b-32c6-42dd-90de-7f3f4b0279d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the files produced by the API crawl\n",
    "if fresh_data:\n",
    "    script_path = repo_path / \"human-wildlife-interactions/src/data/combine\"\n",
    "    input_dir = repo_path / \"intermediate/api_data\"\n",
    "    output_dir = repo_path / \"intermediate/combined\"\n",
    "    if not output_dir.exists():\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    # because this is being run in Great Lakes we can't set as relative paths unfortunately\n",
    "    %run /nfs/turbo/seas-nhcarter/human_wildlife_interactions/repo/human-wildlife-interactions/src/data/combine /nfs/turbo/seas-nhcarter/human_wildlife_interactions/repo/intermediate/api_data /nfs/turbo/seas-nhcarter/human_wildlife_interactions/repo/intermediate/combined\n",
    "else:\n",
    "    print(\"Using existing combined data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2f378e-db89-41fc-9857-79cdfaa06aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This step takes an eternity, only do if it really is necessary\n",
    "if fresh_data:\n",
    "    # translate the title for topic modeling\n",
    "    translated_dir = repo_path / \"intermediate/translated\"\n",
    "    if not translated_dir.exists():\n",
    "        translated_dir.mkdir(parents=True, exist_ok=True)\n",
    "    # replace this with relative paths once it is in the correct directory\n",
    "    %run /nfs/turbo/seas-nhcarter/human_wildlife_interactions/repo/human-wildlife-interactions/src/features/translation /nfs/turbo/seas-nhcarter/human_wildlife_interactions/repo/intermediate/combined/videoDets.pkl snippet.title title /nfs/turbo/seas-nhcarter/human_wildlife_interactions/repo/intermediate/combined/translatedTitle.pkl\n",
    "    \n",
    "    # translate the description for topic modeling\n",
    "    %run /nfs/turbo/seas-nhcarter/human_wildlife_interactions/repo/human-wildlife-interactions/src/features/translation /nfs/turbo/seas-nhcarter/human_wildlife_interactions/repo/intermediate/combined/translatedTitle.pkl snippet.description descrip /nfs/turbo/seas-nhcarter/human_wildlife_interactions/repo/intermediate/combined/desc_title_translated.pkl\n",
    "    !rm /nfs/turbo/seas-nhcarter/human_wildlife_interactions/repo/intermediate/combined/translatedTitle.pkl\n",
    "else:\n",
    "    print(\"Using existing translated data.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
