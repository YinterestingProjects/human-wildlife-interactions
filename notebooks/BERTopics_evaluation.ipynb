{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNLYojvOy8iO5tFHgvaGq1n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YinterestingProjects/human-wildlife-interactions/blob/main/notebooks/BERTopics_evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jB4OfEY00Pek",
        "outputId": "513b73b3-e1fc-4226-d995-769a44744497"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install pandas==1.4.2 bertopic==0.14.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZb-KDmO0bZf",
        "outputId": "4e9fc387-efc9-4c1a-e137-7c76fb80ef84"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pandas==1.4.2\n",
            "  Downloading pandas-1.4.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bertopic==0.14.1\n",
            "  Downloading bertopic-0.14.1-py2.py3-none-any.whl (120 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.7/120.7 KB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas==1.4.2) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas==1.4.2) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.9/dist-packages (from pandas==1.4.2) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2.post1 in /usr/local/lib/python3.9/dist-packages (from bertopic==0.14.1) (1.2.1)\n",
            "Requirement already satisfied: plotly>=4.7.0 in /usr/local/lib/python3.9/dist-packages (from bertopic==0.14.1) (5.5.0)\n",
            "Collecting hdbscan>=0.8.29\n",
            "  Downloading hdbscan-0.8.29.tar.gz (5.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sentence-transformers>=0.4.1\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 KB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.41.1 in /usr/local/lib/python3.9/dist-packages (from bertopic==0.14.1) (4.65.0)\n",
            "Collecting umap-learn>=0.5.0\n",
            "  Downloading umap-learn-0.5.3.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.2/88.2 KB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cython>=0.27 in /usr/local/lib/python3.9/dist-packages (from hdbscan>=0.8.29->bertopic==0.14.1) (0.29.33)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.9/dist-packages (from hdbscan>=0.8.29->bertopic==0.14.1) (1.2.0)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.9/dist-packages (from hdbscan>=0.8.29->bertopic==0.14.1) (1.10.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from plotly>=4.7.0->bertopic==0.14.1) (8.2.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from plotly>=4.7.0->bertopic==0.14.1) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.22.2.post1->bertopic==0.14.1) (3.1.0)\n",
            "Collecting transformers<5.0.0,>=4.6.0\n",
            "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m92.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from sentence-transformers>=0.4.1->bertopic==0.14.1) (1.13.1+cu116)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from sentence-transformers>=0.4.1->bertopic==0.14.1) (0.14.1+cu116)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (from sentence-transformers>=0.4.1->bertopic==0.14.1) (3.7)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0\n",
            "  Downloading huggingface_hub-0.13.1-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 KB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numba>=0.49 in /usr/local/lib/python3.9/dist-packages (from umap-learn>=0.5.0->bertopic==0.14.1) (0.56.4)\n",
            "Collecting pynndescent>=0.5\n",
            "  Downloading pynndescent-0.5.8.tar.gz (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic==0.14.1) (4.5.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic==0.14.1) (6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic==0.14.1) (23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic==0.14.1) (3.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic==0.14.1) (2.25.1)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.9/dist-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic==0.14.1) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic==0.14.1) (57.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic==0.14.1) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk->sentence-transformers>=0.4.1->bertopic==0.14.1) (8.1.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->sentence-transformers>=0.4.1->bertopic==0.14.1) (8.4.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic==0.14.1) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic==0.14.1) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic==0.14.1) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic==0.14.1) (1.26.14)\n",
            "Building wheels for collected packages: hdbscan, sentence-transformers, umap-learn, pynndescent\n",
            "  Building wheel for hdbscan (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hdbscan: filename=hdbscan-0.8.29-cp39-cp39-linux_x86_64.whl size=3582125 sha256=ded1b82946fb4d99e4377d2b1fe3fddb052d766d3cbe6b96dcd3b7ed2793445f\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/6f/88/1a4c04276b98306f00217a1e300e6ba0252c6aa4f7616067ae\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=b7ea1b79330d39e87b5da91d2eafae3bbf19714a0232be7ee09326fc745ad91c\n",
            "  Stored in directory: /root/.cache/pip/wheels/71/67/06/162a3760c40d74dd40bc855d527008d26341c2b0ecf3e8e11f\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.3-py3-none-any.whl size=82829 sha256=781ee60bc02d647bd665fd2e1a9dc28422b193299556a0dd3e2375ff71641153\n",
            "  Stored in directory: /root/.cache/pip/wheels/f4/3e/1c/596d0a463d17475af648688443fa4846fef624d1390339e7e9\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pynndescent: filename=pynndescent-0.5.8-py3-none-any.whl size=55513 sha256=c1312748e0f1f970e1f39f847cac280c1657c645ff1b0016574a3a673ea9a62e\n",
            "  Stored in directory: /root/.cache/pip/wheels/b9/89/cc/59ab91ef5b21dc2ab3635528d7d227f49dfc9169905dcb959d\n",
            "Successfully built hdbscan sentence-transformers umap-learn pynndescent\n",
            "Installing collected packages: tokenizers, sentencepiece, pandas, huggingface-hub, transformers, pynndescent, hdbscan, umap-learn, sentence-transformers, bertopic\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.3.5\n",
            "    Uninstalling pandas-1.3.5:\n",
            "      Successfully uninstalled pandas-1.3.5\n",
            "Successfully installed bertopic-0.14.1 hdbscan-0.8.29 huggingface-hub-0.13.1 pandas-1.4.2 pynndescent-0.5.8 sentence-transformers-2.2.2 sentencepiece-0.1.97 tokenizers-0.13.2 transformers-4.26.1 umap-learn-0.5.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "from bertopic import BERTopic\n",
        "from umap import UMAP\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "# directory setup\n",
        "user = \"sally\"\n",
        "#user = \"jina\"\n",
        "#user = \"josh\"\n",
        "     \n",
        "try:\n",
        "  if user == \"sally\":\n",
        "    directory = '/content/drive/MyDrive/MADS/MADS_Capstone'\n",
        "  elif user == \"jina\":\n",
        "    directory = \"JINA: INSERT YOUR PATH HERE\"\n",
        "  elif user == \"josh\":\n",
        "    directory = \"/content/drive/MyDrive/MADS_Capstone\"\n",
        "except:\n",
        "  dprint(\"No user has been selected\")"
      ],
      "metadata": {
        "id": "_fsJVawK0by1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fp = f'{directory}/data/processed/desc_title_translated.pkl'\n",
        "corpus = pd.read_pickle(fp)"
      ],
      "metadata": {
        "id": "wgH0cGGs09_8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_null(df, verbose=True):\n",
        "    '''filter null translations'''\n",
        "    clean_corp = df[(df != ' ') & (df != '') & (df != 'source language unavailable for translation')]  \n",
        "    if verbose:\n",
        "        print(f'{len(df) - len(clean_corp)} null records dropped out of {len(df)}')  \n",
        "    return clean_corp"
      ],
      "metadata": {
        "id": "pGMVmxsk2ElO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fp = f'{directory}/data/processed/desc_title_translated.pkl'\n",
        "corpus = pd.read_pickle(fp)\n",
        "  \n",
        "order = corpus.index\n",
        "new_doc = filter_null(corpus['title_en']).reset_index()\n",
        "docs = new_doc['title_en']\n",
        "docs\n",
        "new_doc['index']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nj2P0mW1pue",
        "outputId": "7aa6c1c7-2061-479b-ad33-e096d09f008b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "26 null records dropped out of 3895\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0          0\n",
              "1          1\n",
              "2          2\n",
              "3          3\n",
              "4          4\n",
              "        ... \n",
              "3864    3890\n",
              "3865    3891\n",
              "3866    3892\n",
              "3867    3893\n",
              "3868    3894\n",
              "Name: index, Length: 3869, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_doc['index'][3868]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BAPlXnYGXrZ",
        "outputId": "c479e850-a4fe-49d9-90db-39423db0444d"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3894"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------- topic modeling results --------------\n",
        "# X [1]loop through search terms\n",
        "# X [2]loop through similarity query threshold\n",
        "# [3]loop through topics that are above threshold\n",
        "# [4]find threshold cut offs of relevant videos from video-topics matix \n",
        "# [5]if video above threshold mark 1, else mark 0\n",
        "\n",
        "# ----------- human annotation results --------------\n",
        "# [1]loop through all available topics\n",
        "# [2]find threshold cut offs of relevant videos from video-topics matix  (0.5) lower the better check n of video total to review\n",
        "# [3][manual] loop through videos, show video, record human annonatation of hunting activity present (1) or not (0)\n",
        "\n",
        "\n",
        "''''         Human Annot.\n",
        "                0  1 \n",
        "              -------\n",
        "            0 |TN|FN|\n",
        " Topic        -------\n",
        " Modeling   1 |FP|TP|\n",
        "              -------\n",
        "''''\n",
        "\n",
        "# threshold vs precision + recal curve\n",
        "# precison = TP/(TP+FP)     \n",
        "# recall = TP/(TP+FN)"
      ],
      "metadata": {
        "id": "_k0HSy201vfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loop through all available topics\n",
        "# find threshold cutoffs of relevant videos from video-topics matix  (0.5) lower the better check n of video total to review\n",
        "\n",
        "{\n",
        "topic1:[(video index, 0.3),(video index, 0.6), (video index, 0.9)],\n",
        "topic1:[(video index, 0.3),(video index, 0.6), (video index, 0.9)],\n",
        "topic1:[(video index, 0.3),(video index, 0.6), (video index, 0.9)],\n",
        "...\n",
        " }\n",
        "\n",
        "# grab video per topic > 0.5 for human labeling\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DdfyBpjY639Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_topic_video_lookup(topics, probs):\n",
        "  video_topic = list(zip(topics, probs))\n",
        "\n",
        "  topic_video_lookup = {}\n",
        "  for v_id, (topic, prob) in enumerate(video_topic):\n",
        "    if topic not in topic_video_lookup:\n",
        "      topic_video_lookup[topic] = [(v_id, prob)]\n",
        "    else:\n",
        "      topic_video_lookup[topic].append((v_id, prob))\n",
        "  return topic_video_lookup"
      ],
      "metadata": {
        "id": "lk0ULoJGGSoN"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_fp = f'{directory}/models/bertopic_trigram'\n",
        "topic_model = BERTopic.load(model_fp)\n",
        "topics, probs = topic_model.fit_transform(docs)"
      ],
      "metadata": {
        "id": "jBx5ScRXGK2T"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trigram_tp_lookup = get_topic_video_lookup(topics, probs)"
      ],
      "metadata": {
        "id": "I-VSvjcEMYdP"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_tp_lookup = get_topic_video_lookup(topics, probs)"
      ],
      "metadata": {
        "id": "Luo2I1fhMTUE"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "guided_tp_lookup = get_topic_video_lookup(topics, probs)"
      ],
      "metadata": {
        "id": "0SZzuCc2LOU9"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'outlier videos count guided {len(guided_tp_lookup[-1])}')\n",
        "print(f'outlier videos count bigram {len(bigram_tp_lookup[-1])}')\n",
        "print(f'outlier videos count trigram {len(trigram_tp_lookup[-1])}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmzdr06kG288",
        "outputId": "ffb62e63-7b18-4cf6-c17d-0706d84f694c"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "outlier videos count guided 1126\n",
            "outlier videos count bigram 1281\n",
            "outlier videos count trigram 1281\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# filter list by threshold 0.5, find total\n",
        "\n",
        "def count_videos(lookup):\n",
        "  count = 0\n",
        "  for topic in lookup:\n",
        "    count += len(lookup[topic]) \n",
        "  return count\n",
        "\n",
        "def count_empty_topics(lookup):\n",
        "  count = 0\n",
        "  for topic in lookup:\n",
        "    if len(lookup[topic]) == 0:\n",
        "      count += 1\n",
        "      #print(topic)\n",
        "    return count\n",
        "\n",
        "def get_videos_by_topic_threshold(lookup, threshold=0.5):\n",
        "  '''filter a video topic lookup based on threshold of topic video '''\n",
        "  \n",
        "  filtered_videos = {}\n",
        "  for topic in lookup:\n",
        "    filtered_videos[topic] = [(v_id, prob) for v_id, prob in lookup[topic] if prob > threshold]\n",
        "    \n",
        "  print(f'videos remaining: {count_videos(filtered_videos)}/{count_videos(lookup)}, empty topic count {count_empty_topics(filtered_videos)}/{count_empty_topics(lookup)}')\n",
        "  return filtered_videos"
      ],
      "metadata": {
        "id": "UOIdSDCYIgBr"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered = get_videos_by_topic_threshold(trigram_tp_lookup, threshold=0.5)\n",
        "filtered = get_videos_by_topic_threshold(trigram_tp_lookup, threshold=0.6)\n",
        "filtered = get_videos_by_topic_threshold(trigram_tp_lookup, threshold=0.7)\n",
        "filtered = get_videos_by_topic_threshold(trigram_tp_lookup, threshold=0.9)\n",
        "filtered = get_videos_by_topic_threshold(trigram_tp_lookup, threshold=0.95)\n",
        "filtered = get_videos_by_topic_threshold(trigram_tp_lookup, threshold=0.999999)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Cywy9HAOvlE",
        "outputId": "a22a267d-cb2f-4fd7-a0b9-9e2553e0cbb5"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "videos remaining: 2330/3869, empty topic count 1/0\n",
            "videos remaining: 2178/3869, empty topic count 1/0\n",
            "videos remaining: 1995/3869, empty topic count 1/0\n",
            "videos remaining: 1510/3869, empty topic count 1/0\n",
            "videos remaining: 1339/3869, empty topic count 1/0\n",
            "videos remaining: 1187/3869, empty topic count 1/0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered = get_videos_by_topic_threshold(bigram_tp_lookup, threshold=0.5)\n",
        "filtered = get_videos_by_topic_threshold(bigram_tp_lookup, threshold=0.6)\n",
        "filtered = get_videos_by_topic_threshold(bigram_tp_lookup, threshold=0.7)\n",
        "filtered = get_videos_by_topic_threshold(bigram_tp_lookup, threshold=0.9)\n",
        "filtered = get_videos_by_topic_threshold(bigram_tp_lookup, threshold=0.95)\n",
        "filtered = get_videos_by_topic_threshold(bigram_tp_lookup, threshold=0.999999)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMrL635oQGJB",
        "outputId": "3874686a-a495-4376-c0fb-3665f2fa99fa"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "videos remaining: 2330/3869, empty topic count 1/0\n",
            "videos remaining: 2178/3869, empty topic count 1/0\n",
            "videos remaining: 1995/3869, empty topic count 1/0\n",
            "videos remaining: 1510/3869, empty topic count 1/0\n",
            "videos remaining: 1339/3869, empty topic count 1/0\n",
            "videos remaining: 1187/3869, empty topic count 1/0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered = get_videos_by_topic_threshold(guided_tp_lookup, threshold=0.5)\n",
        "filtered = get_videos_by_topic_threshold(guided_tp_lookup, threshold=0.6)\n",
        "filtered = get_videos_by_topic_threshold(guided_tp_lookup, threshold=0.7)\n",
        "filtered = get_videos_by_topic_threshold(guided_tp_lookup, threshold=0.9)\n",
        "filtered = get_videos_by_topic_threshold(guided_tp_lookup, threshold=0.95)\n",
        "filtered = get_videos_by_topic_threshold(guided_tp_lookup, threshold=0.999999)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGjpTxCTSvDN",
        "outputId": "c155ab0a-1344-43eb-c98e-9dcc900c4a4e"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "videos remaining: 2316/3869, empty topic count 1/0\n",
            "videos remaining: 2074/3869, empty topic count 1/0\n",
            "videos remaining: 1850/3869, empty topic count 1/0\n",
            "videos remaining: 1227/3869, empty topic count 1/0\n",
            "videos remaining: 1052/3869, empty topic count 1/0\n",
            "videos remaining: 858/3869, empty topic count 1/0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# randomly sample 300? "
      ],
      "metadata": {
        "id": "I7eLLs4TTXOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_video_index(filtered_lookup, index_key):\n",
        "  '''updating video indexes of the filtered topic video look to original video dataframe index'''\n",
        "\n",
        "  updated_lookup = {}\n",
        "  for topic in filtered_lookup:\n",
        "    video_list = []\n",
        "    for v_id, prob in filtered_lookup[topic]:\n",
        "      video_list.append((index_key[v_id],prob))\n",
        "    updated_lookup[topic] = video_list\n",
        "  \n",
        "  return updated_lookup "
      ],
      "metadata": {
        "id": "RpBKdWJmUvQN"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered[1][:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYvzFHNHWLWf",
        "outputId": "19107ca0-5f64-4adc-f20e-7ace04f89de2"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(4, 1.0),\n",
              " (5, 1.0),\n",
              " (195, 1.0),\n",
              " (451, 1.0),\n",
              " (566, 1.0),\n",
              " (619, 1.0),\n",
              " (629, 1.0),\n",
              " (707, 1.0),\n",
              " (780, 1.0),\n",
              " (790, 1.0)]"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "updated_filtered = update_video_index(filtered, new_doc['index'])\n",
        "updated_filtered[1][:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgonYwwOWEGr",
        "outputId": "278eefba-5190-477f-b77b-66f73235979d"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(4, 1.0),\n",
              " (5, 1.0),\n",
              " (200, 1.0),\n",
              " (456, 1.0),\n",
              " (571, 1.0),\n",
              " (624, 1.0),\n",
              " (634, 1.0),\n",
              " (714, 1.0),\n",
              " (787, 1.0),\n",
              " (797, 1.0)]"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NNrTxpngWYU7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}