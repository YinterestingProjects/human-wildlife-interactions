{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN2QA0l3rhB4KsGcpT1h5WF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YinterestingProjects/human-wildlife-interactions/blob/main/notebooks/BERTopics_evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jB4OfEY00Pek",
        "outputId": "513b73b3-e1fc-4226-d995-769a44744497"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install pandas==1.4.2 bertopic==0.14.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZb-KDmO0bZf",
        "outputId": "4e9fc387-efc9-4c1a-e137-7c76fb80ef84"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pandas==1.4.2\n",
            "  Downloading pandas-1.4.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bertopic==0.14.1\n",
            "  Downloading bertopic-0.14.1-py2.py3-none-any.whl (120 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.7/120.7 KB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas==1.4.2) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas==1.4.2) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.9/dist-packages (from pandas==1.4.2) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2.post1 in /usr/local/lib/python3.9/dist-packages (from bertopic==0.14.1) (1.2.1)\n",
            "Requirement already satisfied: plotly>=4.7.0 in /usr/local/lib/python3.9/dist-packages (from bertopic==0.14.1) (5.5.0)\n",
            "Collecting hdbscan>=0.8.29\n",
            "  Downloading hdbscan-0.8.29.tar.gz (5.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sentence-transformers>=0.4.1\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 KB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.41.1 in /usr/local/lib/python3.9/dist-packages (from bertopic==0.14.1) (4.65.0)\n",
            "Collecting umap-learn>=0.5.0\n",
            "  Downloading umap-learn-0.5.3.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.2/88.2 KB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cython>=0.27 in /usr/local/lib/python3.9/dist-packages (from hdbscan>=0.8.29->bertopic==0.14.1) (0.29.33)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.9/dist-packages (from hdbscan>=0.8.29->bertopic==0.14.1) (1.2.0)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.9/dist-packages (from hdbscan>=0.8.29->bertopic==0.14.1) (1.10.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from plotly>=4.7.0->bertopic==0.14.1) (8.2.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from plotly>=4.7.0->bertopic==0.14.1) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.22.2.post1->bertopic==0.14.1) (3.1.0)\n",
            "Collecting transformers<5.0.0,>=4.6.0\n",
            "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m92.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from sentence-transformers>=0.4.1->bertopic==0.14.1) (1.13.1+cu116)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from sentence-transformers>=0.4.1->bertopic==0.14.1) (0.14.1+cu116)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (from sentence-transformers>=0.4.1->bertopic==0.14.1) (3.7)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0\n",
            "  Downloading huggingface_hub-0.13.1-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 KB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numba>=0.49 in /usr/local/lib/python3.9/dist-packages (from umap-learn>=0.5.0->bertopic==0.14.1) (0.56.4)\n",
            "Collecting pynndescent>=0.5\n",
            "  Downloading pynndescent-0.5.8.tar.gz (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic==0.14.1) (4.5.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic==0.14.1) (6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic==0.14.1) (23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic==0.14.1) (3.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic==0.14.1) (2.25.1)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.9/dist-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic==0.14.1) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic==0.14.1) (57.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic==0.14.1) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk->sentence-transformers>=0.4.1->bertopic==0.14.1) (8.1.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->sentence-transformers>=0.4.1->bertopic==0.14.1) (8.4.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic==0.14.1) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic==0.14.1) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic==0.14.1) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic==0.14.1) (1.26.14)\n",
            "Building wheels for collected packages: hdbscan, sentence-transformers, umap-learn, pynndescent\n",
            "  Building wheel for hdbscan (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hdbscan: filename=hdbscan-0.8.29-cp39-cp39-linux_x86_64.whl size=3582125 sha256=ded1b82946fb4d99e4377d2b1fe3fddb052d766d3cbe6b96dcd3b7ed2793445f\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/6f/88/1a4c04276b98306f00217a1e300e6ba0252c6aa4f7616067ae\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=b7ea1b79330d39e87b5da91d2eafae3bbf19714a0232be7ee09326fc745ad91c\n",
            "  Stored in directory: /root/.cache/pip/wheels/71/67/06/162a3760c40d74dd40bc855d527008d26341c2b0ecf3e8e11f\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.3-py3-none-any.whl size=82829 sha256=781ee60bc02d647bd665fd2e1a9dc28422b193299556a0dd3e2375ff71641153\n",
            "  Stored in directory: /root/.cache/pip/wheels/f4/3e/1c/596d0a463d17475af648688443fa4846fef624d1390339e7e9\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pynndescent: filename=pynndescent-0.5.8-py3-none-any.whl size=55513 sha256=c1312748e0f1f970e1f39f847cac280c1657c645ff1b0016574a3a673ea9a62e\n",
            "  Stored in directory: /root/.cache/pip/wheels/b9/89/cc/59ab91ef5b21dc2ab3635528d7d227f49dfc9169905dcb959d\n",
            "Successfully built hdbscan sentence-transformers umap-learn pynndescent\n",
            "Installing collected packages: tokenizers, sentencepiece, pandas, huggingface-hub, transformers, pynndescent, hdbscan, umap-learn, sentence-transformers, bertopic\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.3.5\n",
            "    Uninstalling pandas-1.3.5:\n",
            "      Successfully uninstalled pandas-1.3.5\n",
            "Successfully installed bertopic-0.14.1 hdbscan-0.8.29 huggingface-hub-0.13.1 pandas-1.4.2 pynndescent-0.5.8 sentence-transformers-2.2.2 sentencepiece-0.1.97 tokenizers-0.13.2 transformers-4.26.1 umap-learn-0.5.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "from bertopic import BERTopic\n",
        "from umap import UMAP\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "# directory setup\n",
        "user = \"sally\"\n",
        "#user = \"jina\"\n",
        "#user = \"josh\"\n",
        "     \n",
        "try:\n",
        "  if user == \"sally\":\n",
        "    directory = '/content/drive/MyDrive/MADS/MADS_Capstone'\n",
        "  elif user == \"jina\":\n",
        "    directory = \"JINA: INSERT YOUR PATH HERE\"\n",
        "  elif user == \"josh\":\n",
        "    directory = \"/content/drive/MyDrive/MADS_Capstone\"\n",
        "except:\n",
        "  dprint(\"No user has been selected\")"
      ],
      "metadata": {
        "id": "_fsJVawK0by1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fp = f'{directory}/data/processed/desc_title_translated.pkl'\n",
        "corpus = pd.read_pickle(fp)"
      ],
      "metadata": {
        "id": "wgH0cGGs09_8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_null(df, verbose=True):\n",
        "    '''filter null translations'''\n",
        "    clean_corp = df[(df != ' ') & (df != '') & (df != 'source language unavailable for translation')]  \n",
        "    if verbose:\n",
        "        print(f'{len(df) - len(clean_corp)} null records dropped out of {len(df)}')  \n",
        "    return clean_corp"
      ],
      "metadata": {
        "id": "pGMVmxsk2ElO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fp = f'{directory}/data/processed/desc_title_translated.pkl'\n",
        "corpus = pd.read_pickle(fp)\n",
        "  \n",
        "order = corpus.index\n",
        "new_doc = filter_null(corpus['title_en']).reset_index()\n",
        "new_doc\n",
        "# docs = new_doc['title_en']\n",
        "# docs\n",
        "new_doc['index']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nj2P0mW1pue",
        "outputId": "65da48d8-9a11-4f37-ca35-dac00e957520"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "26 null records dropped out of 3895\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0          0\n",
              "1          1\n",
              "2          2\n",
              "3          3\n",
              "4          4\n",
              "        ... \n",
              "3864    3890\n",
              "3865    3891\n",
              "3866    3892\n",
              "3867    3893\n",
              "3868    3894\n",
              "Name: index, Length: 3869, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------- topic modeling results --------------\n",
        "# X [1]loop through search terms\n",
        "# X [2]loop through similarity query threshold\n",
        "# [3]loop through topics that are above threshold\n",
        "# [4]find threshold cut offs of relevant videos from video-topics matix \n",
        "# [5]if video above threshold mark 1, else mark 0\n",
        "\n",
        "# ----------- human annotation results --------------\n",
        "# [1]loop through all available topics\n",
        "# [2]find threshold cut offs of relevant videos from video-topics matix  (0.5) lower the better check n of video total to review\n",
        "# [3][manual] loop through videos, show video, record human annonatation of hunting activity present (1) or not (0)\n",
        "\n",
        "\n",
        "# threshold vs precision + recal curve\n",
        "# precison = TP/(TP+FP)     \n",
        "# recall = TP/(TP+FN)"
      ],
      "metadata": {
        "id": "_k0HSy201vfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loop through all available topics\n",
        "# find threshold cut offs of relevant videos from video-topics matix  (0.5) lower the better check n of video total to review\n",
        "\n",
        "{\n",
        "topic1:[(video index, 0.3),(video index, 0.6), (video index, 0.9)],\n",
        "topic1:[(video index, 0.3),(video index, 0.6), (video index, 0.9)],\n",
        "topic1:[(video index, 0.3),(video index, 0.6), (video index, 0.9)],\n",
        "...\n",
        " }\n",
        "\n",
        "# grab video per topic > 0.5 for human labeling\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DdfyBpjY639Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}